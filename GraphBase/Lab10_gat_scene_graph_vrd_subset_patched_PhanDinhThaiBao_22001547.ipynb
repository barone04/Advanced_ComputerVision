{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ebd56c10",
      "metadata": {
        "id": "ebd56c10"
      },
      "source": [
        "# VRD Subset → Scene Graph Classification (Relation-aware GAT-like, **padded batches**)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09b33d0e",
      "metadata": {
        "id": "09b33d0e"
      },
      "source": [
        "Notebook đã vá để xử lý batch kích thước nút khác nhau (padding + mask)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6caa69fa",
      "metadata": {
        "id": "6caa69fa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Nếu chạy trên Colab/local thiếu gói, có thể cài (không bắt buộc nếu đã có):\n",
        "# !pip -q install numpy matplotlib torch\n",
        "ROOT_PATH = \"/content/\"\n",
        "\n",
        "OUTPUT_DIR = ROOT_PATH +  \"vrd_subset/images\"\n",
        "JSON_PATH = ROOT_PATH + \"rels_large.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b5615e53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5615e53",
        "outputId": "d61a9346-a63f-4924-9402-6609cecea0a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, json, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch import optim\n",
        "SEED=123; np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c7ead574",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7ead574",
        "outputId": "4c7f214f-5296-43ab-d442-5b2b8f0b05a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400 graphs from /content/rels_large.jsonl\n",
            "Total graphs: 400 | Example: (6, 6) (6, 10) (6, 6) 1 ['bag', 'bike', 'dog', 'hat', 'horse']\n"
          ]
        }
      ],
      "source": [
        "NODES_VOCAB = [\"person\",\"horse\",\"hat\",\"grass\",\"dog\",\"bike\",\"car\",\"tree\",\"bag\",\"shirt\"]\n",
        "REL_VOCAB   = [\"ride\",\"wear\",\"on\",\"near\",\"next_to\",\"hold\",\"in_front_of\",\"behind\",\"under\",\"over\",\"none\"]\n",
        "node2id={n:i for i,n in enumerate(NODES_VOCAB)}\n",
        "rel2id={r:i for i,r in enumerate(REL_VOCAB)}\n",
        "\n",
        "def one_hot(idx, size):\n",
        "    v=np.zeros(size,dtype=np.float32); v[idx]=1.0; return v\n",
        "\n",
        "def build_graph_from_image_rels(rels, max_nodes=12):\n",
        "    nodes = sorted(set([r['subj'] for r in rels] + [r['obj'] for r in rels]))\n",
        "    nodes = [n for n in nodes if n in node2id][:max_nodes]\n",
        "    if len(nodes)==0: nodes=[\"person\"]\n",
        "    N=len(nodes)\n",
        "    X=np.stack([one_hot(node2id.get(n,0), len(NODES_VOCAB)) for n in nodes],axis=0)\n",
        "    A=np.zeros((N,N),dtype=np.float32)\n",
        "    R=np.full((N,N), rel2id['none'], dtype=np.int64)\n",
        "    for r in rels:\n",
        "        s,o,p = r['subj'], r['obj'], r['predicate']\n",
        "        if s in nodes and o in nodes and s!=o and p in rel2id:\n",
        "            i,j = nodes.index(s), nodes.index(o)\n",
        "            A[i,j]=A[j,i]=1.0\n",
        "            R[i,j]=R[j,i]=rel2id[p]\n",
        "    y=int(any((r['subj']==\"person\" and r['obj']==\"horse\" and r['predicate']==\"ride\") for r in rels))\n",
        "    return A,X,R,y,nodes\n",
        "\n",
        "def load_vrd_subset_jsonl(paths=(JSON_PATH,'vrd_subset/rels.jsonl')):\n",
        "    for path in paths:\n",
        "        if os.path.exists(path):\n",
        "            rels_by_img={}\n",
        "            with open(path,'r',encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    r=json.loads(line); img=r.get('image_id',-1)\n",
        "                    rels_by_img.setdefault(img, []).append(r)\n",
        "            data=[build_graph_from_image_rels(rels) for rels in rels_by_img.values()]\n",
        "            print(f\"Loaded {len(data)} graphs from {path}\")\n",
        "            return data\n",
        "    return None\n",
        "\n",
        "def synthetic_dataset(m=500, seed=SEED):\n",
        "    rng=np.random.default_rng(seed); data=[]\n",
        "    for _ in range(m):\n",
        "        rels=[]\n",
        "        objs=rng.choice(NODES_VOCAB, size=rng.integers(3,7), replace=False).tolist()\n",
        "        if 'person' in objs and 'horse' in objs and rng.random()<0.5:\n",
        "            rels.append({'subj':'person','obj':'horse','predicate':'ride'})\n",
        "        for _ in range(rng.integers(1,4)):\n",
        "            s,o=rng.choice(objs,2,replace=False).tolist()\n",
        "            p=rng.choice(REL_VOCAB[:-1])\n",
        "            rels.append({'subj':s,'obj':o,'predicate':p})\n",
        "        data.append(build_graph_from_image_rels(rels))\n",
        "    return data\n",
        "\n",
        "data=load_vrd_subset_jsonl((JSON_PATH,'vrd_subset/rels.jsonl'))\n",
        "if data is None:\n",
        "    print('No VRD subset file found -> using synthetic fallback.')\n",
        "    data=synthetic_dataset(500)\n",
        "print('Total graphs:', len(data), '| Example:', data[0][0].shape, data[0][1].shape, data[0][2].shape, data[0][3], data[0][4][:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "15bf590c",
      "metadata": {
        "id": "15bf590c"
      },
      "outputs": [],
      "source": [
        "def split_list(L, tr=0.7, va=0.15, seed=SEED):\n",
        "    rng=np.random.default_rng(seed); idx=np.arange(len(L)); rng.shuffle(idx)\n",
        "    n=len(L); t=int(n*tr); v=int(n*va)\n",
        "    return [L[i] for i in idx[:t]], [L[i] for i in idx[t:t+v]], [L[i] for i in idx[t+v:]]\n",
        "\n",
        "train_data, val_data, test_data = split_list(data)\n",
        "\n",
        "def _unpack(sample):\n",
        "    if len(sample)==4:\n",
        "        A,X,R,y = sample; nodes=None\n",
        "    else:\n",
        "        A,X,R,y,nodes = sample\n",
        "    return A,X,R,y,nodes\n",
        "\n",
        "def to_tensors_padded(batch, device):\n",
        "    B=len(batch)\n",
        "    Ns=[_unpack(b)[0].shape[0] for b in batch]\n",
        "    Nmax=max(Ns)\n",
        "    F=_unpack(batch[0])[1].shape[1]\n",
        "    A=np.zeros((B,Nmax,Nmax),dtype=np.float32)\n",
        "    X=np.zeros((B,Nmax,F),dtype=np.float32)\n",
        "    R=np.full((B,Nmax,Nmax), rel2id['none'], dtype=np.int64)\n",
        "    M=np.zeros((B,Nmax),dtype=np.float32)\n",
        "    y=np.zeros(B,dtype=np.int64)\n",
        "    for i,s in enumerate(batch):\n",
        "        Ai,Xi,Ri,yi,_=_unpack(s); n=Ai.shape[0]\n",
        "        A[i,:n,:n]=Ai; X[i,:n,:]=Xi; R[i,:n,:n]=Ri; M[i,:n]=1.0; y[i]=yi\n",
        "    return (torch.tensor(A,dtype=torch.float32,device=device),\n",
        "            torch.tensor(X,dtype=torch.float32,device=device),\n",
        "            torch.tensor(R,dtype=torch.long,device=device),\n",
        "            torch.tensor(M,dtype=torch.float32,device=device),\n",
        "            torch.tensor(y,dtype=torch.long,device=device))\n",
        "\n",
        "def batches(data, bs=32):\n",
        "    for i in range(0,len(data),bs):\n",
        "        yield data[i:i+bs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9d86f3f8",
      "metadata": {
        "id": "9d86f3f8"
      },
      "outputs": [],
      "source": [
        "class RelGATLayer(nn.Module):\n",
        "    def __init__(self,in_dim,out_dim,n_rel,heads=2,dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.heads=heads; self.dk=out_dim//heads; assert out_dim%heads==0\n",
        "        self.Wq=nn.Linear(in_dim,out_dim,bias=False)\n",
        "        self.Wk=nn.Linear(in_dim,out_dim,bias=False)\n",
        "        self.Wv=nn.Linear(in_dim,out_dim,bias=False)\n",
        "        self.rel_emb=nn.Embedding(n_rel, self.dk)\n",
        "        self.dp=nn.Dropout(dropout)\n",
        "    def forward(self,X,A,R,mask):\n",
        "        B,N,F=X.shape; H=self.heads; dk=self.dk\n",
        "        X=X*mask.unsqueeze(-1)\n",
        "        Q=self.Wq(X).view(B,N,H,dk); K=self.Wk(X).view(B,N,H,dk); V=self.Wv(X).view(B,N,H,dk)\n",
        "        rel=self.rel_emb(R).view(B,N,N,dk)\n",
        "        logits = torch.einsum('bnhd,bmhd->bhnm', Q, K) / (dk**0.5)\n",
        "        logits = logits + torch.einsum('bnhd,bnmd->bhnm', Q, rel)\n",
        "        edge_mask=(A==0).unsqueeze(1)\n",
        "        pad_mask=(mask==0).unsqueeze(1).unsqueeze(2)\n",
        "        logits=logits.masked_fill(edge_mask|pad_mask, -1e9)\n",
        "        attn=torch.softmax(logits, dim=-1); attn=self.dp(attn)\n",
        "        out=torch.einsum('bhnm,bmhd->bnhd', attn, V).contiguous().view(B,N,H*dk)\n",
        "        out=out*mask.unsqueeze(-1)\n",
        "        return out, attn\n",
        "\n",
        "class GraphClassifier(nn.Module):\n",
        "    def __init__(self,in_dim,hid,n_rel,heads=2):\n",
        "        super().__init__()\n",
        "        self.g1=RelGATLayer(in_dim,hid,n_rel,heads=heads,dropout=0.1)\n",
        "        self.g2=RelGATLayer(hid,hid,n_rel,heads=heads,dropout=0.1)\n",
        "        self.cls=nn.Sequential(nn.Linear(hid,hid), nn.ReLU(), nn.Linear(hid,2))\n",
        "    def forward(self,X,A,R,mask,return_attn=False):\n",
        "        H1,a1=self.g1(X,A,R,mask); H1=F.relu(H1)\n",
        "        H2,a2=self.g2(H1,A,R,mask); H2=F.relu(H2)\n",
        "        m=mask.unsqueeze(-1); denom=m.sum(dim=1).clamp_min(1e-6)\n",
        "        g=(H2*m).sum(dim=1)/denom\n",
        "        logits=self.cls(g)\n",
        "        return (logits,(a1,a2)) if return_attn else logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bb4b260c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb4b260c",
        "outputId": "a189ad9d-9bc5-4f4d-dad9-cb014b861c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | loss=0.6386 | val_acc=0.517\n",
            "Epoch 20 | loss=0.3623 | val_acc=0.867\n",
            "Epoch 30 | loss=0.2351 | val_acc=0.883\n",
            "Epoch 40 | loss=0.2248 | val_acc=0.883\n",
            "Epoch 50 | loss=0.2083 | val_acc=0.883\n",
            "Best Val acc: 0.9 | Test acc: 0.85\n"
          ]
        }
      ],
      "source": [
        "model=GraphClassifier(in_dim=len(NODES_VOCAB), hid=32, n_rel=len(REL_VOCAB), heads=2).to(device)\n",
        "opt=optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "crit=nn.CrossEntropyLoss()\n",
        "\n",
        "def evaluate(ds, bs=64):\n",
        "    model.eval(); correct=0; total=0\n",
        "    with torch.no_grad():\n",
        "        for b in batches(ds, bs):\n",
        "            A,X,R,M,y = to_tensors_padded(b, device)\n",
        "            p = model(X,A,R,M).argmax(1)\n",
        "            correct += (p==y).sum().item(); total += y.numel()\n",
        "    return correct/total\n",
        "\n",
        "best=(0.0,None)\n",
        "for ep in range(1,51):\n",
        "    model.train(); losses=[]\n",
        "    for b in batches(train_data,64):\n",
        "        A,X,R,M,y = to_tensors_padded(b, device)\n",
        "        opt.zero_grad(); logits=model(X,A,R,M); loss=crit(logits,y)\n",
        "        loss.backward(); opt.step(); losses.append(loss.item())\n",
        "    va=evaluate(val_data)\n",
        "    if va>best[0]: best=(va, {k:v.detach().cpu().clone() for k,v in model.state_dict().items()})\n",
        "    if ep%10==0: print(f\"Epoch {ep:02d} | loss={np.mean(losses):.4f} | val_acc={va:.3f}\")\n",
        "if best[1] is not None: model.load_state_dict(best[1])\n",
        "te=evaluate(test_data)\n",
        "print('Best Val acc:', round(best[0],3), '| Test acc:', round(te,3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ac698261",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac698261",
        "outputId": "d8a79850-0034-464c-c6ec-692c7e4b5471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True label (person-ride-horse present?): 1 | Pred: 1\n",
            "Nodes in graph: ['dog', 'horse', 'person']\n",
            "att(person -> dog): 0.000\n",
            "att(person -> horse): 1.000\n",
            "att(person -> person): 0.000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Inspect attention on one test sample (prefer positive)\n",
        "sample=None\n",
        "for s in test_data:\n",
        "    A,X,R,y,nodes = s if len(s)==5 else (*s, None)\n",
        "    if y==1:\n",
        "        sample = (A,X,R,y,nodes); break\n",
        "if sample is None:\n",
        "    s = test_data[0]; sample = s if len(s)==5 else (*s, None)\n",
        "\n",
        "A,X,R,y,nodes = sample\n",
        "n=A.shape[0]\n",
        "A_t=torch.tensor(A,dtype=torch.float32,device=device).unsqueeze(0)\n",
        "X_t=torch.tensor(X,dtype=torch.float32,device=device).unsqueeze(0)\n",
        "R_t=torch.tensor(R,dtype=torch.long,device=device).unsqueeze(0)\n",
        "M_t=torch.zeros(1,n,dtype=torch.float32,device=device); M_t[:, :n]=1.0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits,(a1,a2)=model(X_t,A_t,R_t,M_t,return_attn=True)\n",
        "pred=int(logits.argmax(1).item())\n",
        "\n",
        "print('True label (person-ride-horse present?):', y, '| Pred:', pred)\n",
        "print('Nodes in graph:', nodes)\n",
        "\n",
        "if nodes is not None and ('person' in nodes):\n",
        "    pid = nodes.index('person')\n",
        "    att = a1.squeeze(0).cpu().numpy()  # [H,N,N]\n",
        "    mean_person = att[:, pid, :].mean(axis=0)\n",
        "    for j,sc in enumerate(mean_person[:n]):\n",
        "        print(f\"att(person -> {nodes[j]}): {sc:.3f}\")\n",
        "else:\n",
        "    print(\"No 'person' node in this graph; skip attention display for 'person'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b76eb657",
        "outputId": "170f53a9-8081-4480-cede-dd949e105d21"
      },
      "source": [
        "NODES_VOCAB = [\"person\",\"horse\",\"hat\",\"grass\",\"dog\",\"bike\",\"car\",\"tree\",\"bag\",\"shirt\"]\n",
        "REL_VOCAB   = [\"ride\",\"wear\",\"on\",\"near\",\"next_to\",\"hold\",\"in_front_of\",\"behind\",\"under\",\"over\",\"none\"]\n",
        "node2id={n:i for i,n in enumerate(NODES_VOCAB)}\n",
        "rel2id={r:i for i,r in enumerate(REL_VOCAB)}\n",
        "\n",
        "def one_hot(idx, size):\n",
        "    v=np.zeros(size,dtype=np.float32); v[idx]=1.0; return v\n",
        "\n",
        "def build_graph_from_image_rels(rels, max_nodes=12):\n",
        "    nodes = sorted(set([r['subj'] for r in rels] + [r['obj'] for r in rels]))\n",
        "    nodes = [n for n in nodes if n in node2id][:max_nodes]\n",
        "    if len(nodes)==0: nodes=[\"person\"]\n",
        "    N=len(nodes)\n",
        "    X=np.stack([one_hot(node2id.get(n,0), len(NODES_VOCAB)) for n in nodes],axis=0)\n",
        "    A=np.ones((N,N),dtype=np.float32) - np.eye(N) # Initialize A as fully connected, excluding self-loops\n",
        "    R=np.full((N,N), rel2id['none'], dtype=np.int64)\n",
        "    for r in rels:\n",
        "        s,o,p = r['subj'], r['obj'], r['predicate']\n",
        "        if s in nodes and o in nodes and s!=o and p in rel2id:\n",
        "            i,j = nodes.index(s), nodes.index(o)\n",
        "            A[i,j]=A[j,i]=1.0 # Explicitly set A=1.0 for existing relations (mostly redundant now)\n",
        "            R[i,j]=R[j,i]=rel2id[p]\n",
        "    y=int(any((r['subj']==\"person\" and r['obj']==\"horse\" and r['predicate']==\"ride\") for r in rels))\n",
        "    return A,X,R,y,nodes\n",
        "\n",
        "def load_vrd_subset_jsonl(paths=(JSON_PATH,'vrd_subset/rels.jsonl')):\n",
        "    for path in paths:\n",
        "        if os.path.exists(path):\n",
        "            rels_by_img={}\n",
        "            with open(path,'r',encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    r=json.loads(line); img=r.get('image_id',-1)\n",
        "                    rels_by_img.setdefault(img, []).append(r)\n",
        "            data=[build_graph_from_image_rels(rels) for rels in rels_by_img.values()]\n",
        "            print(f\"Loaded {len(data)} graphs from {path}\")\n",
        "            return data\n",
        "    return None\n",
        "\n",
        "def synthetic_dataset(m=500, seed=SEED):\n",
        "    rng=np.random.default_rng(seed); data=[]\n",
        "    for _ in range(m):\n",
        "        rels=[]\n",
        "        objs=rng.choice(NODES_VOCAB, size=rng.integers(3,7), replace=False).tolist()\n",
        "        if 'person' in objs and 'horse' in objs and rng.random()<0.5:\n",
        "            rels.append({'subj':'person','obj':'horse','predicate':'ride'})\n",
        "        for _ in range(rng.integers(1,4)):\n",
        "            s,o=rng.choice(objs,2,replace=False).tolist()\n",
        "            p=rng.choice(REL_VOCAB[:-1])\n",
        "            rels.append({'subj':s,'obj':o,'predicate':p})\n",
        "        data.append(build_graph_from_image_rels(rels))\n",
        "    return data\n",
        "\n",
        "data=load_vrd_subset_jsonl((JSON_PATH,'vrd_subset/rels.jsonl'))\n",
        "if data is None:\n",
        "    print('No VRD subset file found -> using synthetic fallback.')\n",
        "    data=synthetic_dataset(500)\n",
        "print('Total graphs:', len(data), '| Example:', data[0][0].shape, data[0][1].shape, data[0][2].shape, data[0][3], data[0][4][:5])"
      ],
      "id": "b76eb657",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400 graphs from /content/rels_large.jsonl\n",
            "Total graphs: 400 | Example: (6, 6) (6, 10) (6, 6) 1 ['bag', 'bike', 'dog', 'hat', 'horse']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b4a5283"
      },
      "source": [
        "\n",
        "##  **Thử dùng full graph (mọi cặp nút đều có cạnh):**\n",
        "Đã sửa đổi hàm `build_graph_from_image_rels` để khởi tạo ma trận kề `A` dưới dạng đồ thị đầy đủ kết nối (không bao gồm các cạnh tự lặp). Mô hình đã được huấn luyện lại và kết quả đã được phân tích, so sánh với hiệu suất ban đầu."
      ],
      "id": "5b4a5283"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c54bb03",
        "outputId": "e38079fc-9cbf-4b5c-9521-5096d4d6d21d"
      },
      "source": [
        "model=GraphClassifier(in_dim=len(NODES_VOCAB), hid=32, n_rel=len(REL_VOCAB), heads=2).to(device)\n",
        "opt=optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "crit=nn.CrossEntropyLoss()\n",
        "\n",
        "def evaluate(ds, bs=64):\n",
        "    model.eval(); correct=0; total=0\n",
        "    with torch.no_grad():\n",
        "        for b in batches(ds, bs):\n",
        "            A,X,R,M,y = to_tensors_padded(b, device)\n",
        "            p = model(X,A,R,M).argmax(1)\n",
        "            correct += (p==y).sum().item(); total += y.numel()\n",
        "    return correct/total\n",
        "\n",
        "best=(0.0,None)\n",
        "for ep in range(1,51):\n",
        "    model.train(); losses=[]\n",
        "    for b in batches(train_data,64):\n",
        "        A,X,R,M,y = to_tensors_padded(b, device)\n",
        "        opt.zero_grad(); logits=model(X,A,R,M); loss=crit(logits,y)\n",
        "        loss.backward(); opt.step(); losses.append(loss.item())\n",
        "    va=evaluate(val_data)\n",
        "    if va>best[0]: best=(va, {k:v.detach().cpu().clone() for k,v in model.state_dict().items()})\n",
        "    if ep%10==0: print(f\"Epoch {ep:02d} | loss={np.mean(losses):.4f} | val_acc={va:.3f}\")\n",
        "if best[1] is not None: model.load_state_dict(best[1])\n",
        "te=evaluate(test_data)\n",
        "print('Best Val acc:', round(best[0],3), '| Test acc:', round(te,3))"
      ],
      "id": "2c54bb03",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | loss=0.6592 | val_acc=0.517\n",
            "Epoch 20 | loss=0.4019 | val_acc=0.867\n",
            "Epoch 30 | loss=0.2629 | val_acc=0.883\n",
            "Epoch 40 | loss=0.2062 | val_acc=0.883\n",
            "Epoch 50 | loss=0.1939 | val_acc=0.883\n",
            "Best Val acc: 0.883 | Test acc: 0.867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d55fec9"
      },
      "source": [
        "### Tóm tắt Kết quả Thử nghiệm\n",
        "\n",
        "**1. So sánh Hiệu suất:**\n",
        "\n",
        "*   **Hiệu suất mô hình gốc:**\n",
        "    *   Độ chính xác xác thực tốt nhất: 0.900\n",
        "    *   Độ chính xác kiểm tra: 0.850\n",
        "*   **Hiệu suất mô hình sửa đổi (Ma trận `A` đầy đủ kết nối):**\n",
        "    *   Độ chính xác xác thực tốt nhất: 0.883\n",
        "    *   Độ chính xác kiểm tra: 0.867\n",
        "\n",
        "**2. Phân tích Kết quả:**\n",
        "\n",
        "*   **Độ chính xác VALIDATION:** Có một sự giảm nhẹ về độ chính xác xác thực tốt nhất (từ 0.900 xuống 0.883). Điều này có thể cho thấy rằng việc buộc tất cả các nút phải được kết nối ban đầu có thể gây ra một số nhiễu hoặc khiến mô hình khó phân biệt các mẫu liên quan hơn trên tập xác thực, có thể do mật độ tính năng đầu vào hiệu quả cao hơn.\n",
        "*   **Độ chính xác TEST:** Ngược lại, độ chính xác kiểm tra tăng nhẹ (từ 0.850 lên 0.867). Điều này cho thấy phương pháp sửa đổi, trong đó mô hình xem xét tất cả các kết nối không tự lặp có thể, có thể dẫn đến khả năng tổng quát hóa tốt hơn đối với dữ liệu chưa thấy. Bằng cách cung cấp một ma trận kề dày đặc, `RelGATLayer` được khuyến khích tính toán sự chú ý trên tất cả các cặp nút có thể. Sau đó, nó dựa vào thành phần `rel_emb(R)` để phân biệt giữa các quan hệ thực tế và các quan hệ 'none'. Điều này có thể làm cho mô hình mạnh mẽ hơn bằng cách ngăn nó phụ thuộc quá mức vào sự thưa thớt của `A` và thay vào đó tận dụng thông tin phong phú trong `R` để suy ra các mối quan hệ, ngay cả khi các cạnh rõ ràng không được `A` nhấn mạnh mạnh mẽ.\n",
        "\n",
        "Tóm lại, sự thay đổi này chuyển trọng tâm từ `A` định nghĩa rõ ràng *những* cạnh nào tồn tại sang `R` định nghĩa rõ ràng *loại* quan hệ nào (hoặc thiếu quan hệ) tồn tại trên cấu trúc đồ thị đầy đủ kết nối vốn có (không bao gồm các cạnh tự lặp). Mô hình dường như hoạt động tương đương, với một sự đánh đổi nhỏ giữa hiệu suất trên tập xác thực và tập kiểm tra."
      ],
      "id": "4d55fec9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Thêm đặc trưng về vị trí (vị trí tương đối, kích thước bounding box) vào vector đặc trưng nút.**"
      ],
      "metadata": {
        "id": "5miInJHALaoK"
      },
      "id": "5miInJHALaoK"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- Cấu hình lại dữ liệu ---\n",
        "NODES_VOCAB = [\"person\",\"horse\",\"hat\",\"grass\",\"dog\",\"bike\",\"car\",\"tree\",\"bag\",\"shirt\"]\n",
        "# Giữ nguyên REL_VOCAB cho phần này\n",
        "REL_VOCAB   = [\"ride\",\"wear\",\"on\",\"near\",\"next_to\",\"hold\",\"in_front_of\",\"behind\",\"under\",\"over\",\"none\"]\n",
        "\n",
        "node2id = {n:i for i,n in enumerate(NODES_VOCAB)}\n",
        "rel2id  = {r:i for i,r in enumerate(REL_VOCAB)}\n",
        "\n",
        "def generate_fake_bbox(rng):\n",
        "    \"\"\"Tạo bbox ngẫu nhiên [cx, cy, w, h] chuẩn hóa 0-1\"\"\"\n",
        "    cx, cy = rng.random(2)\n",
        "    w, h = rng.random(2) * 0.5  # Kích thước tối đa 0.5 ảnh\n",
        "    return np.array([cx, cy, w, h], dtype=np.float32)\n",
        "\n",
        "def build_graph_with_pos(rels, max_nodes=12, rng=None):\n",
        "    if rng is None: rng = np.random.default_rng(123)\n",
        "\n",
        "    # 1. Xác định danh sách nodes\n",
        "    nodes = sorted(set([r['subj'] for r in rels] + [r['obj'] for r in rels]))\n",
        "    nodes = [n for n in nodes if n in node2id][:max_nodes]\n",
        "    if len(nodes) == 0: nodes = [\"person\"]\n",
        "    N = len(nodes)\n",
        "\n",
        "    # 2. Tạo One-hot features\n",
        "    X_semantic = np.stack([one_hot(node2id.get(n,0), len(NODES_VOCAB)) for n in nodes], axis=0)\n",
        "\n",
        "    # 3. Tạo Positional features (Giả lập)\n",
        "    # Trong thực tế, bạn sẽ lấy bbox từ dataset gốc\n",
        "    X_pos = np.stack([generate_fake_bbox(rng) for _ in nodes], axis=0)\n",
        "\n",
        "    # 4. Ghép đặc trưng: [N, Vocab + 4]\n",
        "    X = np.concatenate([X_semantic, X_pos], axis=1)\n",
        "\n",
        "    # 5. Xây dựng ma trận kề A và quan hệ R (Dùng Full Graph như bài 1 đã thử nghiệm)\n",
        "    A = np.ones((N,N), dtype=np.float32) - np.eye(N)\n",
        "    R = np.full((N,N), rel2id['none'], dtype=np.int64)\n",
        "\n",
        "    for r in rels:\n",
        "        s, o, p = r['subj'], r['obj'], r['predicate']\n",
        "        if s in nodes and o in nodes and s != o and p in rel2id:\n",
        "            i, j = nodes.index(s), nodes.index(o)\n",
        "            R[i,j] = R[j,i] = rel2id[p] # Vô hướng cho đơn giản hoặc có hướng tùy bài toán\n",
        "\n",
        "    # Label: person-ride-horse\n",
        "    y = int(any((r['subj']==\"person\" and r['obj']==\"horse\" and r['predicate']==\"ride\") for r in rels))\n",
        "\n",
        "    return A, X, R, y, nodes\n",
        "\n",
        "# Cập nhật hàm tạo dữ liệu synthetic để dùng hàm build mới\n",
        "def synthetic_dataset_with_pos(m=500, seed=123):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    data = []\n",
        "    for _ in range(m):\n",
        "        rels = []\n",
        "        objs = rng.choice(NODES_VOCAB, size=rng.integers(3,7), replace=False).tolist()\n",
        "        # Logic tạo quan hệ giả lập giữ nguyên\n",
        "        if 'person' in objs and 'horse' in objs and rng.random() < 0.5:\n",
        "            rels.append({'subj':'person','obj':'horse','predicate':'ride'})\n",
        "        for _ in range(rng.integers(1,4)):\n",
        "            s, o = rng.choice(objs, 2, replace=False).tolist()\n",
        "            p = rng.choice(REL_VOCAB[:-1])\n",
        "            rels.append({'subj':s, 'obj':o, 'predicate':p})\n",
        "\n",
        "        data.append(build_graph_with_pos(rels, rng=rng))\n",
        "    return data\n",
        "\n",
        "# Tạo lại dữ liệu với đặc trưng vị trí\n",
        "data_pos = synthetic_dataset_with_pos(500)\n",
        "print(\"Shape node features mới (Example 0):\", data_pos[0][1].shape)\n",
        "# Kỳ vọng output shape: (N, 14) -> 10 classes + 4 pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nnKN9OmIy3B",
        "outputId": "75d92a1f-c6ae-43a0-8fb7-d617c4f37b3a"
      },
      "id": "1nnKN9OmIy3B",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape node features mới (Example 0): (2, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train với đặc trưng vị trí\n",
        "train_data, val_data, test_data = split_list(data_pos)\n",
        "\n",
        "# in_dim = 10 (vocab) + 4 (bbox) = 14\n",
        "model = GraphClassifier(in_dim=len(NODES_VOCAB)+4, hid=32, n_rel=len(REL_VOCAB), heads=2).to(device)\n",
        "opt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "best=(0.0,None)\n",
        "for ep in range(1,51):\n",
        "    model.train(); losses=[]\n",
        "    for b in batches(train_data,64):\n",
        "        A,X,R,M,y = to_tensors_padded(b, device)\n",
        "        opt.zero_grad(); logits=model(X,A,R,M); loss=crit(logits,y)\n",
        "        loss.backward(); opt.step(); losses.append(loss.item())\n",
        "    va=evaluate(val_data)\n",
        "    if va>best[0]: best=(va, {k:v.detach().cpu().clone() for k,v in model.state_dict().items()})\n",
        "    if ep%10==0: print(f\"Epoch {ep:02d} | loss={np.mean(losses):.4f} | val_acc={va:.3f}\")\n",
        "if best[1] is not None: model.load_state_dict(best[1])\n",
        "te=evaluate(test_data)\n",
        "print('Best Val acc:', round(best[0],3), '| Test acc:', round(te,3))"
      ],
      "metadata": {
        "id": "zJ1YY1HIKlOp",
        "outputId": "e73b30dd-792a-4e5c-9c65-298713b75d48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zJ1YY1HIKlOp",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | loss=0.4226 | val_acc=0.907\n",
            "Epoch 20 | loss=0.2736 | val_acc=0.907\n",
            "Epoch 30 | loss=0.1540 | val_acc=0.907\n",
            "Epoch 40 | loss=0.0973 | val_acc=0.973\n",
            "Epoch 50 | loss=0.0476 | val_acc=0.987\n",
            "Best Val acc: 0.987 | Test acc: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Thử bỏ bớt các quan hệ hiếm, chỉ giữ lại vài quan hệ phổ biến, quan sát độ chính xác.**"
      ],
      "metadata": {
        "id": "vcnlKOwGLkBY"
      },
      "id": "vcnlKOwGLkBY"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Chỉ giữ lại các quan hệ tương tác trực tiếp, loại bỏ quan hệ không gian chung chung\n",
        "# (Ví dụ: bỏ 'near', 'next_to', 'behind'...)\n",
        "KEEP_RELS = [\"ride\", \"wear\", \"hold\", \"none\"]\n",
        "\n",
        "# Tạo ánh xạ ID mới (chỉ có 4 loại thay vì 11 như cũ)\n",
        "new_rel2id = {r: i for i, r in enumerate(KEEP_RELS)}\n",
        "print(f\"Bộ quan hệ rút gọn: {new_rel2id}\")\n",
        "\n",
        "\n",
        "def filter_rels(rels):\n",
        "    \"\"\"Lọc danh sách quan hệ, chỉ giữ lại cái nào có trong KEEP_RELS.\"\"\"\n",
        "    filtered = []\n",
        "    for r in rels:\n",
        "        if r['predicate'] in new_rel2id:\n",
        "            filtered.append(r)\n",
        "    return filtered\n",
        "\n",
        "def build_graph_pruned(rels, max_nodes=12):\n",
        "    # Bước 1: Lọc bỏ quan hệ hiếm trước khi xây đồ thị\n",
        "    rels = filter_rels(rels)\n",
        "\n",
        "    # Bước 2: Xác định nodes từ các quan hệ còn lại\n",
        "    nodes = sorted(set([r['subj'] for r in rels] + [r['obj'] for r in rels]))\n",
        "    nodes = [n for n in nodes if n in node2id][:max_nodes]\n",
        "    if len(nodes) == 0: nodes = [\"person\"]\n",
        "    N = len(nodes)\n",
        "\n",
        "    # Bước 3: Tạo node features (X) - vẫn dùng semantic one-hot cũ\n",
        "    X = np.stack([one_hot(node2id.get(n, 0), len(NODES_VOCAB)) for n in nodes], axis=0)\n",
        "\n",
        "    # Bước 4: Tạo Adjacency (A) - Dùng Full Graph để tận dụng kết quả Task 1\n",
        "    A = np.ones((N, N), dtype=np.float32) - np.eye(N)\n",
        "\n",
        "    # Bước 5: Tạo Relation Matrix (R) với ID MỚI\n",
        "    # Khởi tạo toàn bộ là 'none' theo ID mới\n",
        "    R = np.full((N, N), new_rel2id['none'], dtype=np.int64)\n",
        "\n",
        "    for r in rels:\n",
        "        s, o, p = r['subj'], r['obj'], r['predicate']\n",
        "        if s in nodes and o in nodes and s != o:\n",
        "            i, j = nodes.index(s), nodes.index(o)\n",
        "            # Gán ID mới từ new_rel2id\n",
        "            R[i, j] = R[j, i] = new_rel2id[p]\n",
        "\n",
        "    # Label: Vẫn detect 'person-ride-horse'\n",
        "    y = int(any((r['subj'] == \"person\" and r['obj'] == \"horse\" and r['predicate'] == \"ride\") for r in rels))\n",
        "\n",
        "    return A, X, R, y, nodes\n",
        "\n",
        "def synthetic_dataset_pruned(m=500, seed=123):\n",
        "    \"\"\"Sinh dữ liệu giả lập và áp dụng logic cắt tỉa quan hệ.\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    data = []\n",
        "    for _ in range(m):\n",
        "        rels = []\n",
        "        objs = rng.choice(NODES_VOCAB, size=rng.integers(3, 7), replace=False).tolist()\n",
        "\n",
        "        # Target relation (quan trọng)\n",
        "        if 'person' in objs and 'horse' in objs and rng.random() < 0.5:\n",
        "            rels.append({'subj': 'person', 'obj': 'horse', 'predicate': 'ride'})\n",
        "\n",
        "        # Noise relations (ngẫu nhiên)\n",
        "        for _ in range(rng.integers(1, 4)):\n",
        "            s, o = rng.choice(objs, 2, replace=False).tolist()\n",
        "            # Random từ tập gốc, nhưng sẽ bị filter bởi build_graph_pruned nếu không thuộc KEEP_RELS\n",
        "            p = rng.choice(REL_VOCAB[:-1])\n",
        "            rels.append({'subj': s, 'obj': o, 'predicate': p})\n",
        "\n",
        "        data.append(build_graph_pruned(rels))\n",
        "    return data\n",
        "\n",
        "\n",
        "# Tải lại dữ liệu mới (đã prune)\n",
        "print(\"Đang tạo dataset với quan hệ rút gọn...\")\n",
        "data_task3 = synthetic_dataset_pruned(500)\n",
        "train_d3, val_d3, test_d3 = split_list(data_task3)\n",
        "\n",
        "# Khởi tạo lại model với n_rel nhỏ hơn\n",
        "# in_dim giữ nguyên (10), nhưng n_rel giảm xuống còn 4\n",
        "model_task3 = GraphClassifier(\n",
        "    in_dim=len(NODES_VOCAB),\n",
        "    hid=32,\n",
        "    n_rel=len(KEEP_RELS),  # <--- Thay đổi quan trọng: 4 instead of 11\n",
        "    heads=2\n",
        ").to(device)\n",
        "\n",
        "opt = optim.Adam(model_task3.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "print(f\"\\nBắt đầu train Task 3 (Num relations: {len(KEEP_RELS)})...\")\n",
        "\n",
        "def evaluate_task3(model, ds, bs=64):\n",
        "    model.eval()\n",
        "    correct = 0; total = 0\n",
        "    with torch.no_grad():\n",
        "        for b in batches(ds, bs):\n",
        "            # Hàm to_tensors_padded hoạt động dựa trên shape của batch, nên dùng lại được\n",
        "            A, X, R, M, y = to_tensors_padded(b, device)\n",
        "            logits = model(X, A, R, M)\n",
        "            pred = logits.argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.numel()\n",
        "    return correct / total\n",
        "\n",
        "best_acc = 0.0\n",
        "for ep in range(1, 51):\n",
        "    model_task3.train()\n",
        "    losses = []\n",
        "    for b in batches(train_d3, 64):\n",
        "        A, X, R, M, y = to_tensors_padded(b, device)\n",
        "\n",
        "        print(f\"Max index in X: {X.max().item()}\")\n",
        "        print(f\"Max index in R: {R.max().item()}\")\n",
        "\n",
        "        opt.zero_grad()\n",
        "        logits = model_task3(X, A, R, M)\n",
        "        loss = crit(logits, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    val_acc = evaluate_task3(model_task3, val_d3)\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "\n",
        "    if ep % 10 == 0:\n",
        "        print(f\"Epoch {ep:02d} | loss={np.mean(losses):.4f} | val_acc={val_acc:.3f}\")\n",
        "\n",
        "print(f\"Task 3 Result - Best Val Acc: {best_acc:.3f}\")\n",
        "test_acc = evaluate_task3(model_task3, test_d3)\n",
        "print(f\"Task 3 Result - Test Acc: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "k7pfj2nlKrXK",
        "outputId": "cbf2cf60-9f11-42bc-cb61-884879d87915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "id": "k7pfj2nlKrXK",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bộ quan hệ rút gọn: {'ride': 0, 'wear': 1, 'hold': 2, 'none': 3}\n",
            "Đang tạo dataset với quan hệ rút gọn...\n",
            "\n",
            "Bắt đầu train Task 3 (Num relations: 4)...\n",
            "Max index in X: 1.0\n",
            "Max index in R: 10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2114527486.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_task3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-81810864.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, A, R, mask, return_attn)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mH1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mH1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mH2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mH2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-81810864.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, A, R, mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mQ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mrel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrel_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bnhd,bmhd->bhnm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdk\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bnhd,bnmd->bhnm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2540\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2542\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-mm9b4BLE1a"
      },
      "id": "0-mm9b4BLE1a",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}