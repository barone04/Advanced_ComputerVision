{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14063656,"sourceType":"datasetVersion","datasetId":8951548}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\n\"\"\"Minimal Pix2Pix toy training for paired A|B images. Run directly with defaults.\"\"\"\n\nimport glob\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom PIL import Image\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nBASE_DIR = \"/kaggle/input/data-36/\"\nDATA_DIR = \"/kaggle/input/data-36/data/pix2pix_toy/train\"\nOUTPUT_DIR = \"/kaggle/working/outputs_pix2pix_toy\"\nEPOCHS = 50\nBATCH_SIZE = 4\nLR = 2e-4\nLAMBDA_L1 = 100.0\nIMG_SIZE = 256\n\n\nclass Pix2PixDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        super().__init__()\n        self.root_dir = root_dir\n        self.paths = sorted(glob.glob(os.path.join(root_dir, \"*.png\")))\n        if len(self.paths) == 0:\n            raise RuntimeError(f\"No .png files found in {root_dir}\")\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, idx):\n        path = self.paths[idx]\n        img = Image.open(path).convert(\"RGB\")\n        w, h = img.size\n        w2 = w // 2\n        img_a = img.crop((0, 0, w2, h))\n        img_b = img.crop((w2, 0, w, h))\n        if self.transform:\n            img_a = self.transform(img_a)\n            img_b = self.transform(img_b)\n        return img_a, img_b\n\n\nclass UNetDown(nn.Module):\n    def __init__(self, in_ch, out_ch, norm=True):\n        super().__init__()\n        layers = [nn.Conv2d(in_ch, out_ch, 4, 2, 1, bias=False)]\n        if norm:\n            layers.append(nn.BatchNorm2d(out_ch))\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass UNetUp(nn.Module):\n    def __init__(self, in_ch, out_ch, dropout=0.0):\n        super().__init__()\n        layers = [\n            nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(True),\n        ]\n        if dropout > 0:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip):\n        x = self.model(x)\n        x = torch.cat([x, skip], dim=1)\n        return x\n\n\nclass UNetGenerator(nn.Module):\n    def __init__(self, in_ch=3, out_ch=3):\n        super().__init__()\n        self.down1 = UNetDown(in_ch, 64, norm=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512)\n        self.down5 = UNetDown(512, 512)\n        self.down6 = UNetDown(512, 512)\n        self.down7 = UNetDown(512, 512, norm=False)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 256)\n        self.up5 = UNetUp(512, 128)\n        self.up6 = UNetUp(256, 64)\n\n        self.last = nn.Sequential(nn.ConvTranspose2d(128, out_ch, 4, 2, 1), nn.Tanh())\n\n    def forward(self, x):\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n\n        u1 = self.up1(d7, d6)\n        u2 = self.up2(u1, d5)\n        u3 = self.up3(u2, d4)\n        u4 = self.up4(u3, d3)\n        u5 = self.up5(u4, d2)\n        u6 = self.up6(u5, d1)\n        out = self.last(u6)\n        return out\n\n\nclass PatchDiscriminator(nn.Module):\n    def __init__(self, in_ch=6):\n        super().__init__()\n\n        def block(in_c, out_c, norm=True):\n            layers = [nn.Conv2d(in_c, out_c, 4, 2, 1, bias=False)]\n            if norm:\n                layers.append(nn.BatchNorm2d(out_c))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(in_ch, 64, norm=False),\n            *block(64, 128),\n            *block(128, 256),\n            nn.Conv2d(256, 1, 4, 1, 1),\n        )\n\n    def forward(self, x, y):\n        inp = torch.cat([x, y], dim=1)\n        return self.model(inp)\n\n\ndef denorm(x):\n    return (x + 1) / 2\n\n\ndef train_pix2pix(\n    data_dir=DATA_DIR,\n    outdir=OUTPUT_DIR,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    lr=LR,\n    lambda_L1=LAMBDA_L1,\n    img_size=IMG_SIZE,\n):\n    os.makedirs(outdir, exist_ok=True)\n\n    transform = transforms.Compose(\n        [\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    )\n\n    dataset = Pix2PixDataset(data_dir, transform=transform)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n\n    G = UNetGenerator(in_ch=3, out_ch=3).to(device)\n    D = PatchDiscriminator(in_ch=6).to(device)\n\n    bce = nn.BCEWithLogitsLoss()\n    l1 = nn.L1Loss()\n\n    optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n    optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n\n    for epoch in range(epochs):\n        for i, (img_a, img_b) in enumerate(loader):\n            img_a = img_a.to(device)\n            img_b = img_b.to(device)\n\n            optimizer_D.zero_grad()\n            fake_b = G(img_a).detach()\n\n            pred_real = D(img_a, img_b)\n            pred_fake = D(img_a, fake_b)\n\n            target_real = torch.ones_like(pred_real)\n            target_fake = torch.zeros_like(pred_fake)\n\n            loss_D_real = bce(pred_real, target_real)\n            loss_D_fake = bce(pred_fake, target_fake)\n            loss_D = (loss_D_real + loss_D_fake) * 0.5\n            loss_D.backward()\n            optimizer_D.step()\n\n            optimizer_G.zero_grad()\n            fake_b = G(img_a)\n            pred_fake_for_G = D(img_a, fake_b)\n            target_real_for_G = torch.ones_like(pred_fake_for_G)\n\n            loss_G_GAN = bce(pred_fake_for_G, target_real_for_G)\n            loss_G_L1 = l1(fake_b, img_b) * lambda_L1\n            loss_G = loss_G_GAN + loss_G_L1\n            loss_G.backward()\n            optimizer_G.step()\n\n            if i % 50 == 0:\n                print(\n                    f\"[Pix2Pix] Epoch [{epoch+1}/{epochs}] \"\n                    f\"Step [{i}/{len(loader)}] \"\n                    f\"Loss_D: {loss_D.item():.4f}, Loss_G: {loss_G.item():.4f}\"\n                )\n\n        G.eval()\n        with torch.no_grad():\n            img_a_sample, img_b_sample = next(iter(loader))\n            img_a_sample = img_a_sample.to(device)\n            img_b_sample = img_b_sample.to(device)\n            fake_b_sample = G(img_a_sample)\n\n            grid = torch.cat([denorm(img_a_sample), denorm(img_b_sample), denorm(fake_b_sample)], dim=0)\n            save_image(grid, os.path.join(outdir, f\"epoch_{epoch+1:03d}.png\"), nrow=img_a_sample.size(0))\n        G.train()\n\n    print(\"Training complete. Images saved to:\", outdir)\n\n\nif __name__ == \"__main__\":\n    train_pix2pix()","metadata":{"_uuid":"c3ba090e-944e-454b-8b13-574841c9b141","_cell_guid":"c6660dcc-19d9-4af2-88e9-26f4bce2c11c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-12-08T17:56:53.413585Z","iopub.execute_input":"2025-12-08T17:56:53.413791Z","iopub.status.idle":"2025-12-08T18:00:24.164044Z","shell.execute_reply.started":"2025-12-08T17:56:53.413772Z","shell.execute_reply":"2025-12-08T18:00:24.162923Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n[Pix2Pix] Epoch [1/50] Step [0/50] Loss_D: 0.7096, Loss_G: 88.7422\n[Pix2Pix] Epoch [2/50] Step [0/50] Loss_D: 0.0943, Loss_G: 41.0403\n[Pix2Pix] Epoch [3/50] Step [0/50] Loss_D: 0.0863, Loss_G: 35.8605\n[Pix2Pix] Epoch [4/50] Step [0/50] Loss_D: 0.6972, Loss_G: 38.1654\n[Pix2Pix] Epoch [5/50] Step [0/50] Loss_D: 0.1545, Loss_G: 31.2015\n[Pix2Pix] Epoch [6/50] Step [0/50] Loss_D: 0.2015, Loss_G: 33.5882\n[Pix2Pix] Epoch [7/50] Step [0/50] Loss_D: 0.1627, Loss_G: 33.9019\n[Pix2Pix] Epoch [8/50] Step [0/50] Loss_D: 0.3330, Loss_G: 28.8964\n[Pix2Pix] Epoch [9/50] Step [0/50] Loss_D: 0.2204, Loss_G: 33.3103\n[Pix2Pix] Epoch [10/50] Step [0/50] Loss_D: 0.2215, Loss_G: 26.0978\n[Pix2Pix] Epoch [11/50] Step [0/50] Loss_D: 0.2143, Loss_G: 28.7904\n[Pix2Pix] Epoch [12/50] Step [0/50] Loss_D: 0.3388, Loss_G: 32.3535\n[Pix2Pix] Epoch [13/50] Step [0/50] Loss_D: 0.4621, Loss_G: 29.5012\n[Pix2Pix] Epoch [14/50] Step [0/50] Loss_D: 0.3156, Loss_G: 19.4465\n[Pix2Pix] Epoch [15/50] Step [0/50] Loss_D: 0.3885, Loss_G: 29.6861\n[Pix2Pix] Epoch [16/50] Step [0/50] Loss_D: 0.5683, Loss_G: 18.6622\n[Pix2Pix] Epoch [17/50] Step [0/50] Loss_D: 0.6013, Loss_G: 20.2091\n[Pix2Pix] Epoch [18/50] Step [0/50] Loss_D: 0.6237, Loss_G: 24.1129\n[Pix2Pix] Epoch [19/50] Step [0/50] Loss_D: 0.1895, Loss_G: 30.0004\n[Pix2Pix] Epoch [20/50] Step [0/50] Loss_D: 0.5859, Loss_G: 16.1650\n[Pix2Pix] Epoch [21/50] Step [0/50] Loss_D: 0.4496, Loss_G: 17.7657\n[Pix2Pix] Epoch [22/50] Step [0/50] Loss_D: 0.5293, Loss_G: 21.3638\n[Pix2Pix] Epoch [23/50] Step [0/50] Loss_D: 0.8608, Loss_G: 15.2373\n[Pix2Pix] Epoch [24/50] Step [0/50] Loss_D: 0.3891, Loss_G: 25.5378\n[Pix2Pix] Epoch [25/50] Step [0/50] Loss_D: 0.5388, Loss_G: 16.1556\n[Pix2Pix] Epoch [26/50] Step [0/50] Loss_D: 0.2254, Loss_G: 30.5482\n[Pix2Pix] Epoch [27/50] Step [0/50] Loss_D: 0.5770, Loss_G: 11.4432\n[Pix2Pix] Epoch [28/50] Step [0/50] Loss_D: 0.5892, Loss_G: 14.6526\n[Pix2Pix] Epoch [29/50] Step [0/50] Loss_D: 0.4623, Loss_G: 20.0555\n[Pix2Pix] Epoch [30/50] Step [0/50] Loss_D: 0.4831, Loss_G: 10.5395\n[Pix2Pix] Epoch [31/50] Step [0/50] Loss_D: 0.6128, Loss_G: 15.6399\n[Pix2Pix] Epoch [32/50] Step [0/50] Loss_D: 0.3753, Loss_G: 14.8269\n[Pix2Pix] Epoch [33/50] Step [0/50] Loss_D: 0.7513, Loss_G: 14.6555\n[Pix2Pix] Epoch [34/50] Step [0/50] Loss_D: 0.6072, Loss_G: 12.1605\n[Pix2Pix] Epoch [35/50] Step [0/50] Loss_D: 0.6077, Loss_G: 12.7457\n[Pix2Pix] Epoch [36/50] Step [0/50] Loss_D: 0.7874, Loss_G: 17.0215\n[Pix2Pix] Epoch [37/50] Step [0/50] Loss_D: 0.6781, Loss_G: 10.7432\n[Pix2Pix] Epoch [38/50] Step [0/50] Loss_D: 0.6838, Loss_G: 8.4774\n[Pix2Pix] Epoch [39/50] Step [0/50] Loss_D: 0.7547, Loss_G: 9.2150\n[Pix2Pix] Epoch [40/50] Step [0/50] Loss_D: 0.3342, Loss_G: 8.3101\n[Pix2Pix] Epoch [41/50] Step [0/50] Loss_D: 0.6962, Loss_G: 10.8433\n[Pix2Pix] Epoch [42/50] Step [0/50] Loss_D: 0.4548, Loss_G: 7.1588\n[Pix2Pix] Epoch [43/50] Step [0/50] Loss_D: 0.2472, Loss_G: 10.0331\n[Pix2Pix] Epoch [44/50] Step [0/50] Loss_D: 0.2468, Loss_G: 8.5018\n[Pix2Pix] Epoch [45/50] Step [0/50] Loss_D: 0.1351, Loss_G: 6.7695\n[Pix2Pix] Epoch [46/50] Step [0/50] Loss_D: 0.6942, Loss_G: 16.2971\n[Pix2Pix] Epoch [47/50] Step [0/50] Loss_D: 0.1407, Loss_G: 11.2325\n[Pix2Pix] Epoch [48/50] Step [0/50] Loss_D: 0.6293, Loss_G: 5.4507\n[Pix2Pix] Epoch [49/50] Step [0/50] Loss_D: 0.6484, Loss_G: 6.0377\n[Pix2Pix] Epoch [50/50] Step [0/50] Loss_D: 0.1028, Loss_G: 7.8359\nTraining complete. Images saved to: /kaggle/working/outputs_pix2pix_toy\n","output_type":"stream"}],"execution_count":1}]}