{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.nn.utils.rnn import pad_sequence\nimport collections\nfrom torch.utils.data import random_split\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Cấu hình thiết bị\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --- 1. Xây dựng Vocabulary ---\nclass Vocabulary:\n    def __init__(self, freq_threshold):\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.freq_threshold = freq_threshold\n\n    def __len__(self):\n        return len(self.itos)\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = collections.Counter()\n        idx = 4\n        \n        for sentence in sentence_list:\n            for word in sentence.lower().split():\n                frequencies[word] += 1\n                \n        for word, count in frequencies.items():\n            if count >= self.freq_threshold:\n                self.stoi[word] = idx\n                self.itos[idx] = word\n                idx += 1\n\n    def numericalize(self, text):\n        tokenized_text = text.lower().split()\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n            for token in tokenized_text\n        ]\n\n# --- 2. Xây dựng Dataset ---\nclass Flickr8kDataset(Dataset):\n    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(captions_file)\n        self.transform = transform\n        \n        # Lấy captions và ảnh\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n        \n        # Xây dựng vocab\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocabulary(self.captions.tolist())\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        caption = self.captions[index]\n        img_id = self.imgs[index]\n        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n\n        return img, torch.tensor(numericalized_caption)\n\n# --- 3. Collate Function (Padding) ---\nclass MyCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs, dim=0)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n        return imgs, targets\n\n# --- Thiết lập Path và Loader ---\n# Lưu ý: Cấu trúc thư mục trên Kaggle thường là /kaggle/input/flickr8k/Images và captions.txt\nimage_folder = '/kaggle/input/flickr8k/Images'\ncaptions_file = '/kaggle/input/flickr8k/captions.txt'\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)), # ResNet chuẩn input 224\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\n# Load dữ liệu (Cần đảm bảo file tồn tại, nếu không code sẽ báo lỗi)\ntry:\n    dataset = Flickr8kDataset(image_folder, captions_file, transform=transform)\n    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n    # train_loader = DataLoader(\n    #     dataset=dataset,\n    #     batch_size=32, # Batch size theo yêu cầu\n    #     num_workers=2,\n    #     shuffle=True,\n    #     collate_fn=MyCollate(pad_idx=pad_idx)\n    # )\n\n    # 1. Xác định kích thước tập train và test (ví dụ: 90% train, 10% test)\n    train_size = int(0.9 * len(dataset))\n    test_size = len(dataset) - train_size\n    \n    # 2. Chia ngẫu nhiên dataset\n    train_set, test_set = random_split(dataset, [train_size, test_size])\n    \n    # 3. Tạo lại Train Loader (nếu muốn train trên đúng tập train_set mới)\n    train_loader = DataLoader(\n        dataset=train_set,\n        batch_size=32,\n        num_workers=2,\n        shuffle=True,\n        collate_fn=MyCollate(pad_idx=pad_idx)\n    )\n    \n    # 4. Tạo Test Loader (đây là biến bạn đang thiếu)\n    test_loader = DataLoader(\n        dataset=test_set,\n        batch_size=32,\n        num_workers=2,\n        shuffle=False, # Test không cần shuffle\n        collate_fn=MyCollate(pad_idx=pad_idx)\n    )\n\n    print(f\"Train size: {len(train_set)}, Test size: {len(test_set)}\")\n    print(f\"Vocab size: {len(dataset.vocab)}\")\nexcept Exception as e:\n    print(f\"Lỗi load data: {e}. Hãy kiểm tra lại đường dẫn file trên Kaggle.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T15:20:58.011689Z","iopub.execute_input":"2025-12-09T15:20:58.012506Z","iopub.status.idle":"2025-12-09T15:20:58.208170Z","shell.execute_reply.started":"2025-12-09T15:20:58.012475Z","shell.execute_reply":"2025-12-09T15:20:58.207507Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTrain size: 36409, Test size: 4046\nVocab size: 3005\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import torchvision.models as models\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.models import ViT_B_16_Weights\n\nclass ViTEncoder(nn.Module):\n    def __init__(self, out_dim=512):\n        super().__init__()\n        # Load pre-trained ViT-B/16\n        weights = ViT_B_16_Weights.DEFAULT\n        self.vit = torchvision.models.vit_b_16(weights=weights)\n        \n        # ViT-B/16 có hidden_dim = 768. \n        # Ta cần chiếu về out_dim (512) để khớp với Decoder LSTM ở Module 1\n        self.proj = nn.Linear(768, out_dim)\n        \n        # Freeze các tầng đầu của ViT để train nhanh hơn (tùy chọn)\n        for param in self.vit.parameters():\n            param.requires_grad = False \n        # Unfreeze projection layer\n        for param in self.proj.parameters():\n            param.requires_grad = True\n\n    def forward(self, x):\n        # x: B x 3 x 224 x 224\n        \n        # 1. Chuyển ảnh thành patch embeddings\n        # Hàm _process_input thực hiện: Conv2d (chia patch) -> Flatten -> Transpose\n        x = self.vit._process_input(x) \n        n = x.shape[0]\n\n        # 2. Thêm Class Token (Learnable)\n        batch_class_token = self.vit.class_token.expand(n, -1, -1)\n        x = torch.cat([batch_class_token, x], dim=1) # B x (1 + 196) x 768 (với ảnh 224x224, patch 16 -> 14x14=196 patches)\n\n        # 3. Cộng Positional Embedding và đi qua Encoder Layers\n        x = self.vit.encoder(x) # B x 197 x 768\n\n        # 4. Chiếu về chiều dữ liệu mong muốn\n        x = self.proj(x) # B x 197 x 512\n\n        # Tách CLS token và Patch tokens\n        cls_token = x[:, 0]     # Global Vector (B x 512)\n        patch_tokens = x[:, 1:] # Memory cho Attention (B x 196 x 512)\n        \n        return cls_token, patch_tokens\n\n# --- Attention ---\nclass AdditiveAttention(nn.Module):\n    def __init__(self, dim_q, dim_k, dim_h):\n        super().__init__()\n        self.Wq = nn.Linear(dim_q, dim_h)\n        self.Wk = nn.Linear(dim_k, dim_h)\n        self.v = nn.Linear(dim_h, 1)\n\n    def forward(self, q, k, mask=None):\n        # q: B x hidden_dim, k: B x L x mem_dim\n        q_ = self.Wq(q).unsqueeze(1)    # B x 1 x dim_h\n        k_ = self.Wk(k)                 # B x L x dim_h\n        \n        # Broadcasting q_ cộng với k_\n        attn_energy = self.v(torch.tanh(q_ + k_)).squeeze(-1) # B x L\n        \n        if mask is not None:\n            attn_energy = attn_energy.masked_fill(mask == 0, -1e9)\n            \n        alpha = torch.softmax(attn_energy, dim=-1) # B x L\n        context = torch.bmm(alpha.unsqueeze(1), k).squeeze(1) # B x mem_dim\n        \n        return context, alpha\n\n# --- Decoder ---\nclass LSTMAttnDecoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, mem_dim):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.lstm = nn.LSTM(emb_dim + mem_dim, hidden_dim, batch_first=True)\n        self.attn = AdditiveAttention(dim_q=hidden_dim, dim_k=mem_dim, dim_h=256) # dim_h=256 theo bài\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        self.hidden_dim = hidden_dim\n\n    def forward(self, captions, memory):\n        # captions: B x T\n        B, T = captions.shape\n        embeddings = self.emb(captions) # B x T x Emb\n        \n        # Khởi tạo hidden state và cell state\n        h = torch.zeros(1, B, self.hidden_dim, device=captions.device)\n        c = torch.zeros(1, B, self.hidden_dim, device=captions.device)\n        \n        outputs = []\n        # Loop qua từng time step (Teacher Forcing)\n        for t in range(T):\n            h_curr = h[-1] # Lấy layer cuối cùng\n            \n            # Tính attention context\n            context, _ = self.attn(h_curr, memory)\n            \n            # Input cho LSTM: ghép embedding từ hiện tại + context vector\n            lstm_input = torch.cat([embeddings[:, t, :], context], dim=1).unsqueeze(1) # B x 1 x (Emb+Mem)\n            \n            out, (h, c) = self.lstm(lstm_input, (h, c))\n            \n            logits = self.fc(out.squeeze(1))\n            outputs.append(logits.unsqueeze(1))\n            \n        return torch.cat(outputs, dim=1)\n\n    def generate_caption(self, memory, vocab, max_len=20):\n        # Hàm dùng cho inference (tạo caption cho 1 ảnh)\n        # memory: 1 x L x D\n        batch_size = memory.size(0)\n        h = torch.zeros(1, batch_size, self.hidden_dim, device=memory.device)\n        c = torch.zeros(1, batch_size, self.hidden_dim, device=memory.device)\n        \n        # Bắt đầu bằng thẻ <SOS>\n        input_word = torch.tensor([vocab.stoi[\"<SOS>\"]], device=memory.device)\n        \n        captions = []\n        attentions = []\n        \n        for _ in range(max_len):\n            embed = self.emb(input_word) # 1 x Emb\n            h_curr = h[-1]\n            \n            context, alpha = self.attn(h_curr, memory)\n            attentions.append(alpha.cpu().detach())\n            \n            lstm_input = torch.cat([embed, context], dim=1).unsqueeze(1)\n            out, (h, c) = self.lstm(lstm_input, (h, c))\n            \n            output = self.fc(out.squeeze(1))\n            predicted_idx = output.argmax(1)\n            \n            captions.append(predicted_idx.item())\n            \n            if predicted_idx.item() == vocab.stoi[\"<EOS>\"]:\n                break\n                \n            input_word = predicted_idx\n            \n        return [vocab.itos[idx] for idx in captions], attentions\n\n# --- Tổng hợp Model ---\nclass ViTLSTMVgModel(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        \n    def forward(self, images, captions):\n        _, memory = self.encoder(images)\n        outputs = self.decoder(captions, memory)\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T15:20:58.209506Z","iopub.execute_input":"2025-12-09T15:20:58.209832Z","iopub.status.idle":"2025-12-09T15:20:58.226415Z","shell.execute_reply.started":"2025-12-09T15:20:58.209808Z","shell.execute_reply":"2025-12-09T15:20:58.225798Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# --- Cấu hình Hyperparameters ---\n# vocab_size lấy từ dataset Module 1\nd_model = 512 \nnhead = 8\nnum_layers = 4 # Giảm xuống 4 để train nhanh hơn trên Kaggle\n# Lưu ý: CNNEncoder output dim phải khớp với d_model của Transformer\n# Nếu CNN ra 2048, cần projection layer trong Encoder về 512\n\n# Khởi tạo\nencoder = CNNEncoder(out_dim=d_model).to(device) # Sử dụng CNNEncoder từ Module 1\ndecoder = TransformerCaptionDecoder(vocab_size, d_model, nhead, num_layers).to(device)\nmodel = ViTLSTMVgModel(encoder, decoder).to(device)\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\noptimizer = optim.Adam(model.parameters(), lr=1e-4) # LR thấp hơn chút cho ViT stable\n\nprint(\"Bắt đầu huấn luyện ViT + LSTM...\")\nmodel.train()\n\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    epoch_loss = 0\n    for idx, (imgs, captions) in enumerate(train_loader):\n        imgs = imgs.to(device)\n        captions = captions.to(device)\n        \n        # Forward\n        outputs = model(imgs, captions[:, :-1])\n        targets = captions[:, 1:]\n        \n        loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n        if idx % 50 == 0:\n            print(f\"Epoch {epoch+1}, Step {idx}, Loss: {loss.item():.4f}\")\n\n    print(f\"End Epoch {epoch+1}, Avg Loss: {epoch_loss/len(train_loader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T15:20:58.227153Z","iopub.execute_input":"2025-12-09T15:20:58.227376Z","iopub.status.idle":"2025-12-09T15:32:38.034311Z","shell.execute_reply.started":"2025-12-09T15:20:58.227350Z","shell.execute_reply":"2025-12-09T15:32:38.033472Z"}},"outputs":[{"name":"stdout","text":"Bắt đầu huấn luyện ViT + LSTM...\nEpoch 1, Step 0, Loss: 8.1721\nEpoch 1, Step 50, Loss: 4.5419\nEpoch 1, Step 100, Loss: 4.3601\nEpoch 1, Step 150, Loss: 3.8829\nEpoch 1, Step 200, Loss: 3.8910\nEpoch 1, Step 250, Loss: 3.6199\nEpoch 1, Step 300, Loss: 3.5239\nEpoch 1, Step 350, Loss: 3.0121\nEpoch 1, Step 400, Loss: 3.3674\nEpoch 1, Step 450, Loss: 3.4003\nEpoch 1, Step 500, Loss: 3.4828\nEpoch 1, Step 550, Loss: 3.1448\nEpoch 1, Step 600, Loss: 3.3945\nEpoch 1, Step 650, Loss: 3.1044\nEpoch 1, Step 700, Loss: 2.8090\nEpoch 1, Step 750, Loss: 2.9782\nEpoch 1, Step 800, Loss: 3.0682\nEpoch 1, Step 850, Loss: 3.1245\nEpoch 1, Step 900, Loss: 3.0733\nEpoch 1, Step 950, Loss: 2.6174\nEpoch 1, Step 1000, Loss: 2.8605\nEpoch 1, Step 1050, Loss: 2.8225\nEpoch 1, Step 1100, Loss: 2.6017\nEnd Epoch 1, Avg Loss: 3.3715\nEpoch 2, Step 0, Loss: 2.6317\nEpoch 2, Step 50, Loss: 2.6602\nEpoch 2, Step 100, Loss: 2.5668\nEpoch 2, Step 150, Loss: 2.6231\nEpoch 2, Step 200, Loss: 2.5983\nEpoch 2, Step 250, Loss: 2.3616\nEpoch 2, Step 300, Loss: 2.8852\nEpoch 2, Step 350, Loss: 2.7198\nEpoch 2, Step 400, Loss: 2.3616\nEpoch 2, Step 450, Loss: 2.3937\nEpoch 2, Step 500, Loss: 2.6633\nEpoch 2, Step 550, Loss: 2.7482\nEpoch 2, Step 600, Loss: 2.7441\nEpoch 2, Step 650, Loss: 2.8489\nEpoch 2, Step 700, Loss: 2.5494\nEpoch 2, Step 750, Loss: 2.4502\nEpoch 2, Step 800, Loss: 2.3319\nEpoch 2, Step 850, Loss: 2.6863\nEpoch 2, Step 900, Loss: 2.4248\nEpoch 2, Step 950, Loss: 2.6331\nEpoch 2, Step 1000, Loss: 2.5936\nEpoch 2, Step 1050, Loss: 2.1820\nEpoch 2, Step 1100, Loss: 2.5406\nEnd Epoch 2, Avg Loss: 2.5775\nEpoch 3, Step 0, Loss: 2.2572\nEpoch 3, Step 50, Loss: 2.3366\nEpoch 3, Step 100, Loss: 2.1492\nEpoch 3, Step 150, Loss: 2.3332\nEpoch 3, Step 200, Loss: 2.4211\nEpoch 3, Step 250, Loss: 1.9872\nEpoch 3, Step 300, Loss: 2.2747\nEpoch 3, Step 350, Loss: 1.9625\nEpoch 3, Step 400, Loss: 2.2900\nEpoch 3, Step 450, Loss: 2.1148\nEpoch 3, Step 500, Loss: 2.3764\nEpoch 3, Step 550, Loss: 2.2193\nEpoch 3, Step 600, Loss: 2.2586\nEpoch 3, Step 650, Loss: 2.0746\nEpoch 3, Step 700, Loss: 2.1285\nEpoch 3, Step 750, Loss: 2.1414\nEpoch 3, Step 800, Loss: 2.2361\nEpoch 3, Step 850, Loss: 2.1798\nEpoch 3, Step 900, Loss: 2.1566\nEpoch 3, Step 950, Loss: 1.9362\nEpoch 3, Step 1000, Loss: 2.1905\nEpoch 3, Step 1050, Loss: 2.2828\nEpoch 3, Step 1100, Loss: 2.3227\nEnd Epoch 3, Avg Loss: 2.2521\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## **Giải đáp câu hỏi phân tích (5.5)**\n\n### **(a) Lợi ích và chi phí của \"Attention ở cả hai phía\" (Encoder Self-Attention & Decoder Cross-Attention):**\n\n    - Lợi ích (Chất lượng):\n\n        Encoder (Self-Attention): Mỗi patch (ví dụ: góc ảnh chứa cái cây) có thể \"nhìn\" thấy và trao đổi thông tin với patch khác (ví dụ: góc ảnh chứa bầu trời) ngay lập tức. Điều này giúp ViT hiểu ngữ cảnh toàn cục (global context) tốt hơn CNN (CNN cần nhiều lớp mới nhìn bao quát được). Feature map đầu ra của ViT giàu ngữ nghĩa hơn.\n\n        Decoder (Cross-Attention): Giúp LSTM chọn lọc thông tin từ kho ngữ nghĩa phong phú đó để sinh từ chính xác.\n\n    - Chi phí (Tính toán):\n\n        Rất nặng. Self-Attention trong Encoder có độ phức tạp O(L^2) (với L là số patch). Cross-Attention trong Decoder có độ phức tạp O(T⋅L) (với T là độ dài câu).\n\n### **(b) Khi số patch L tăng (Ví dụ: ảnh to hơn hoặc patch size nhỏ đi):**\n\n    - Tại Encoder (ViT): Chi phí tăng theo bình phương O(L^2).\n\n        Ví dụ: Nếu giảm patch size từ 16 xuống 8 → số patch tăng gấp 4 → chi phí tính toán attention tăng gấp 16 lần. Đây là điểm yếu chí tử của ViT thuần.\n\n    - Tại Decoder (LSTM+Attn): Chi phí tăng tuyến tính O(L).\n\n        Decoder chỉ cần tính tích vô hướng với L vector memory mỗi bước. Việc tăng L không gây áp lực quá lớn cho Decoder so với Encoder.","metadata":{}}]}