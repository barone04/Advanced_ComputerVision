{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.nn.utils.rnn import pad_sequence\nimport collections\nfrom torch.utils.data import random_split\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Cấu hình thiết bị\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --- 1. Xây dựng Vocabulary ---\nclass Vocabulary:\n    def __init__(self, freq_threshold):\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.freq_threshold = freq_threshold\n\n    def __len__(self):\n        return len(self.itos)\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = collections.Counter()\n        idx = 4\n        \n        for sentence in sentence_list:\n            for word in sentence.lower().split():\n                frequencies[word] += 1\n                \n        for word, count in frequencies.items():\n            if count >= self.freq_threshold:\n                self.stoi[word] = idx\n                self.itos[idx] = word\n                idx += 1\n\n    def numericalize(self, text):\n        tokenized_text = text.lower().split()\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n            for token in tokenized_text\n        ]\n\n# --- 2. Xây dựng Dataset ---\nclass Flickr8kDataset(Dataset):\n    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(captions_file)\n        self.transform = transform\n        \n        # Lấy captions và ảnh\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n        \n        # Xây dựng vocab\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocabulary(self.captions.tolist())\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        caption = self.captions[index]\n        img_id = self.imgs[index]\n        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n\n        return img, torch.tensor(numericalized_caption)\n\n# --- 3. Collate Function (Padding) ---\nclass MyCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs, dim=0)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n        return imgs, targets\n\n# --- Thiết lập Path và Loader ---\n# Lưu ý: Cấu trúc thư mục trên Kaggle thường là /kaggle/input/flickr8k/Images và captions.txt\nimage_folder = '/kaggle/input/flickr8k/Images'\ncaptions_file = '/kaggle/input/flickr8k/captions.txt'\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)), # ResNet chuẩn input 224\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\n# Load dữ liệu (Cần đảm bảo file tồn tại, nếu không code sẽ báo lỗi)\ntry:\n    dataset = Flickr8kDataset(image_folder, captions_file, transform=transform)\n    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n    # train_loader = DataLoader(\n    #     dataset=dataset,\n    #     batch_size=32, # Batch size theo yêu cầu\n    #     num_workers=2,\n    #     shuffle=True,\n    #     collate_fn=MyCollate(pad_idx=pad_idx)\n    # )\n\n    # 1. Xác định kích thước tập train và test (ví dụ: 90% train, 10% test)\n    train_size = int(0.9 * len(dataset))\n    test_size = len(dataset) - train_size\n    \n    # 2. Chia ngẫu nhiên dataset\n    train_set, test_set = random_split(dataset, [train_size, test_size])\n    \n    # 3. Tạo lại Train Loader (nếu muốn train trên đúng tập train_set mới)\n    train_loader = DataLoader(\n        dataset=train_set,\n        batch_size=32,\n        num_workers=2,\n        shuffle=True,\n        collate_fn=MyCollate(pad_idx=pad_idx)\n    )\n    \n    # 4. Tạo Test Loader (đây là biến bạn đang thiếu)\n    test_loader = DataLoader(\n        dataset=test_set,\n        batch_size=32,\n        num_workers=2,\n        shuffle=False, # Test không cần shuffle\n        collate_fn=MyCollate(pad_idx=pad_idx)\n    )\n\n    print(f\"Train size: {len(train_set)}, Test size: {len(test_set)}\")\n    print(f\"Vocab size: {len(dataset.vocab)}\")\nexcept Exception as e:\n    print(f\"Lỗi load data: {e}. Hãy kiểm tra lại đường dẫn file trên Kaggle.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T14:59:14.221585Z","iopub.execute_input":"2025-12-09T14:59:14.222271Z","iopub.status.idle":"2025-12-09T14:59:14.410202Z","shell.execute_reply.started":"2025-12-09T14:59:14.222247Z","shell.execute_reply":"2025-12-09T14:59:14.409380Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTrain size: 36409, Test size: 4046\nVocab size: 3005\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import torchvision.models as models\n\n# --- Encoder ---\nclass CNNEncoder(nn.Module):\n    def __init__(self, out_dim=512):\n        super().__init__()\n        # Sử dụng weights mặc định thay vì pretrained=True (cú pháp mới)\n        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n        \n        # Bỏ avgpool và fc layers cuối cùng\n        self.cnn = nn.Sequential(*list(resnet.children())[:-2])\n        self.proj = nn.Conv2d(2048, out_dim, kernel_size=1)\n        \n    def forward(self, x):\n        f = self.cnn(x)  # B x 2048 x 7 x 7\n        f = self.proj(f) # B x out_dim x 7 x 7\n        B, D, H, W = f.shape\n        \n        # Flatten feature map thành memory cho attention\n        memory = f.flatten(2).permute(0, 2, 1) # B x (H*W) x D\n        global_vec = f.mean(dim=[2, 3])        # B x D\n        return global_vec, memory\n\n# --- 1. Positional Encoding ---\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        # Tạo ma trận PE\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1).float()\n        \n        # Công thức: div_term = 10000^(2i/d_model)\n        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        \n        # Thêm batch dimension: 1 x max_len x D\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        # x: B x L x D\n        L = x.size(1)\n        # Cộng PE vào embedding, slice theo độ dài thực tế\n        return x + self.pe[:, :L, :]\n\n# --- 2. Transformer Decoder ---\nclass TransformerCaptionDecoder(nn.Module):\n    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, max_len=100):\n        super().__init__()\n        \n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model, max_len=max_len)\n        \n        # Decoder Layer chuẩn của PyTorch\n        # batch_first=True giúp input/output có dạng [Batch, Seq, Dim]\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=d_model, \n            nhead=nhead, \n            dim_feedforward=2048, # Thường gấp 4 lần d_model\n            dropout=0.1,\n            batch_first=True \n        )\n        \n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(d_model, vocab_size)\n        self.d_model = d_model\n\n    def generate_square_subsequent_mask(self, sz, device):\n        # Tạo mask che phần tương lai (triangular mask)\n        # Giá trị -inf để softmax về 0, hoặc True/False tuỳ version\n        mask = torch.triu(torch.ones(sz, sz, device=device) * float('-inf'), diagonal=1)\n        return mask\n\n    def forward(self, captions, memory, tgt_padding_mask=None):\n        # captions: B x T (Target input)\n        # memory: B x L x D (Image features from CNN)\n        \n        # 1. Embedding + Positional Encoding\n        tgt = self.emb(captions) * math.sqrt(self.d_model) # Scale embedding\n        tgt = self.pos(tgt)\n        \n        # 2. Tạo Causal Mask (Che tương lai)\n        T = tgt.size(1)\n        tgt_mask = self.generate_square_subsequent_mask(T, tgt.device)\n        \n        # 3. Transformer Decoder Pass\n        # memory_key_padding_mask=None vì ảnh có kích thước cố định (49 vị trí)\n        output = self.decoder(\n            tgt=tgt, \n            memory=memory, \n            tgt_mask=tgt_mask,\n            tgt_key_padding_mask=tgt_padding_mask \n        )\n        \n        # 4. Dự đoán từ\n        logits = self.fc(output) # B x T x Vocab\n        return logits\n\nclass TransformerImageCaptioning(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        \n    def forward(self, images, captions):\n        # 1. Encoder ảnh\n        _, memory = self.encoder(images) # Memory: B x 49 x 512\n        \n        # 2. Tạo padding mask cho caption (True ở vị trí <PAD>)\n        # captions: B x T\n        tgt_padding_mask = (captions == 0) # Giả sử <PAD> index là 0\n        \n        # 3. Decoder\n        logits = self.decoder(captions, memory, tgt_padding_mask=tgt_padding_mask)\n        return logits\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T14:41:30.695423Z","iopub.execute_input":"2025-12-09T14:41:30.695635Z","iopub.status.idle":"2025-12-09T14:41:30.708169Z","shell.execute_reply.started":"2025-12-09T14:41:30.695619Z","shell.execute_reply":"2025-12-09T14:41:30.707453Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# --- Cấu hình Hyperparameters ---\n# vocab_size lấy từ dataset Module 1\nd_model = 512 \nnhead = 8\nnum_layers = 4 # Giảm xuống 4 để train nhanh hơn trên Kaggle\n# Lưu ý: CNNEncoder output dim phải khớp với d_model của Transformer\n# Nếu CNN ra 2048, cần projection layer trong Encoder về 512\n\n# Khởi tạo\nencoder = CNNEncoder(out_dim=d_model).to(device) # Sử dụng CNNEncoder từ Module 1\ndecoder = TransformerCaptionDecoder(vocab_size, d_model, nhead, num_layers).to(device)\nmodel = TransformerImageCaptioning(encoder, decoder).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4) # Transformer nhạy cảm với LR, nên để thấp\ncriterion = nn.CrossEntropyLoss(ignore_index=0) # Bỏ qua padding khi tính loss\n\n# --- Training Loop ---\nprint(\"Start Training Transformer...\")\nmodel.train()\nfor epoch in range(3): # Train 3 epochs\n    epoch_loss = 0\n    for idx, (imgs, captions) in enumerate(train_loader):\n        imgs = imgs.to(device)\n        captions = captions.to(device)\n        \n        # Input cho decoder: <SOS> ... w_n\n        decoder_input = captions[:, :-1]\n        \n        # Target thực tế: w_1 ... <EOS>\n        targets = captions[:, 1:]\n        \n        # Forward\n        logits = model(imgs, decoder_input)\n        \n        # Flatten để tính Loss\n        loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n        \n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Clip grad norm để tránh bùng nổ gradient\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        if idx % 100 == 0:\n            print(f\"Epoch {epoch+1}, Step {idx}, Loss: {loss.item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T14:59:20.621507Z","iopub.execute_input":"2025-12-09T14:59:20.622183Z","iopub.status.idle":"2025-12-09T15:11:05.129842Z","shell.execute_reply.started":"2025-12-09T14:59:20.622158Z","shell.execute_reply":"2025-12-09T15:11:05.128925Z"}},"outputs":[{"name":"stdout","text":"Start Training Transformer...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Step 0, Loss: 8.1379\nEpoch 1, Step 100, Loss: 3.8233\nEpoch 1, Step 200, Loss: 3.4597\nEpoch 1, Step 300, Loss: 3.5062\nEpoch 1, Step 400, Loss: 3.1531\nEpoch 1, Step 500, Loss: 3.2052\nEpoch 1, Step 600, Loss: 3.3836\nEpoch 1, Step 700, Loss: 3.0974\nEpoch 1, Step 800, Loss: 2.9501\nEpoch 1, Step 900, Loss: 2.9694\nEpoch 1, Step 1000, Loss: 2.8555\nEpoch 1, Step 1100, Loss: 2.8381\nEpoch 2, Step 0, Loss: 2.8825\nEpoch 2, Step 100, Loss: 3.0786\nEpoch 2, Step 200, Loss: 2.5478\nEpoch 2, Step 300, Loss: 2.7205\nEpoch 2, Step 400, Loss: 2.7810\nEpoch 2, Step 500, Loss: 2.3729\nEpoch 2, Step 600, Loss: 2.5837\nEpoch 2, Step 700, Loss: 2.5517\nEpoch 2, Step 800, Loss: 2.4081\nEpoch 2, Step 900, Loss: 2.5578\nEpoch 2, Step 1000, Loss: 2.6398\nEpoch 2, Step 1100, Loss: 2.5490\nEpoch 3, Step 0, Loss: 2.2933\nEpoch 3, Step 100, Loss: 2.0413\nEpoch 3, Step 200, Loss: 2.2975\nEpoch 3, Step 300, Loss: 2.4056\nEpoch 3, Step 400, Loss: 2.3527\nEpoch 3, Step 500, Loss: 2.4921\nEpoch 3, Step 600, Loss: 2.1674\nEpoch 3, Step 700, Loss: 2.3772\nEpoch 3, Step 800, Loss: 2.1885\nEpoch 3, Step 900, Loss: 2.6681\nEpoch 3, Step 1000, Loss: 2.3723\nEpoch 3, Step 1100, Loss: 2.1234\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# from torchtext.data.metrics import bleu_score\n# Hoặc dùng nltk nếu torchtext bản mới đổi API:\nfrom nltk.translate.bleu_score import corpus_bleu\n\ndef evaluate_bleu(model, dataloader, device, vocab):\n    model.eval()\n    refs = []  # Danh sách caption gốc\n    hyps = []  # Danh sách caption dự đoán\n    \n    with torch.no_grad():\n        for idx, (imgs, captions) in enumerate(dataloader):\n            if idx > 20: break # Test nhanh trên 20 batch đầu\n            imgs = imgs.to(device)\n            \n            # Encoder\n            _, memory = model.encoder(imgs)\n            \n            # Autoregressive Generation\n            # Bắt đầu với <SOS>\n            batch_size = imgs.size(0)\n            ys = torch.ones(batch_size, 1).fill_(vocab.stoi[\"<SOS>\"]).type(torch.long).to(device)\n            \n            for i in range(20): # Max length caption\n                tgt_mask = model.decoder.generate_square_subsequent_mask(ys.size(1), device)\n                \n                # Gọi decoder với chuỗi hiện tại ys\n                out = model.decoder.decoder(\n                    model.decoder.pos(model.decoder.emb(ys) * math.sqrt(d_model)), \n                    memory, \n                    tgt_mask=tgt_mask\n                )\n                \n                # Lấy logit của từ cuối cùng\n                prob = model.decoder.fc(out[:, -1])\n                _, next_word = torch.max(prob, dim=1)\n                \n                ys = torch.cat([ys, next_word.unsqueeze(1)], dim=1)\n            \n            # Convert IDs to Words\n            for j in range(batch_size):\n                # Xử lý caption gốc\n                ref_tokens = [vocab.itos[t.item()] for t in captions[j] if t.item() not in [0, 1, 2]]\n                refs.append([ref_tokens]) # NLTK cần list of lists cho refs\n                \n                # Xử lý caption dự đoán\n                pred_tokens = [vocab.itos[t.item()] for t in ys[j] if t.item() not in [0, 1, 2]]\n                hyps.append(pred_tokens)\n\n    # Tính BLEU\n    bleu4 = corpus_bleu(refs, hyps, weights=(0.25, 0.25, 0.25, 0.25))\n    print(f\"BLEU-4 Score: {bleu4*100:.2f}\")\n\n# Gọi hàm đánh giá (lưu ý: chạy hơi lâu vì generation tuần tự)\nevaluate_bleu(model, test_loader, device, dataset.vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T15:11:40.423003Z","iopub.execute_input":"2025-12-09T15:11:40.423699Z","iopub.status.idle":"2025-12-09T15:11:45.020048Z","shell.execute_reply.started":"2025-12-09T15:11:40.423663Z","shell.execute_reply":"2025-12-09T15:11:45.019162Z"}},"outputs":[{"name":"stdout","text":"BLEU-4 Score: 8.04\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}