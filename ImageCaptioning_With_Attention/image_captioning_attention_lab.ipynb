{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "71d5b0f2",
      "metadata": {
        "id": "71d5b0f2"
      },
      "source": [
        "# Image Captioning với Attention – Lab Notebook\n",
        "\n",
        "Notebook này minh họa các pipeline Image Captioning với **Encoder–Decoder có Attention**,\n",
        "áp dụng cho các lựa chọn sau (chọn trong phần cấu hình):\n",
        "\n",
        "1. `cnn_lstm_attn`: Encoder CNN – Decoder LSTM + Attention (có thể dùng GloVe).\n",
        "2. `cnn_tf`: Encoder CNN – Decoder Transformer.\n",
        "3. `vit_tf`: Encoder ViT – Decoder Transformer.\n",
        "4. `vit_tf_gnn`: Như `vit_tf` nhưng bổ sung thông tin từ Graph NN nhẹ.\n",
        "\n",
        "Các bước chính trong notebook:\n",
        "\n",
        "1. Đọc dữ liệu caption Flickr8K, tiền xử lý, tạo vocab.\n",
        "2. Tạo input phần ngôn ngữ (chuỗi token).\n",
        "3. Dùng Encoder (pretrained) để trích xuất feature vector / memory từ ảnh.\n",
        "4. Kết hợp Attention để tạo context vector chuyển sang Decoder.\n",
        "5. Huấn luyện Decoder (phần ngôn ngữ).\n",
        "6. Tính BLEU trên tập validation.\n",
        "7. Chạy inference trên 1 hoặc nhiều ảnh.\n",
        "8. Trực quan hóa chú ý (attention heatmap / mask) trên ảnh tương ứng caption sinh ra.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c404b9e",
      "metadata": {
        "id": "8c404b9e"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# CẤU HÌNH CHUNG\n",
        "# =========================\n",
        "import os\n",
        "\n",
        "# CHỌN KIẾN TRÚC:\n",
        "# 'cnn_lstm_attn', 'cnn_tf', 'vit_tf', 'vit_tf_gnn'\n",
        "ARCH = 'cnn_lstm_attn'\n",
        "\n",
        "# Đường dẫn dữ liệu Flickr8K\n",
        "DATA_ROOT = '/kaggle/input/flickr8k'  # sửa lại cho phù hợp môi trường\n",
        "IMG_DIR = os.path.join(DATA_ROOT, 'Images')\n",
        "CAPTION_FILE = os.path.join(DATA_ROOT, 'Flickr8k_captions.txt')\n",
        "\n",
        "# Đường dẫn file GloVe (chỉ dùng cho ARCH = 'cnn_lstm_attn' nếu muốn)\n",
        "GLOVE_PATH = '/kaggle/input/glove-6b/glove.6B.50d.txt'  # sửa lại nếu cần\n",
        "USE_GLOVE = True  # đặt False nếu không có file GloVe\n",
        "\n",
        "# Cấu hình train\n",
        "BATCH_SIZE = 32\n",
        "MAX_LEN = 20\n",
        "FREQ_MIN = 5\n",
        "VOCAB_MAX_SIZE = 10000\n",
        "EMB_DIM = 256     # sẽ override nếu dùng GloVe 50d\n",
        "HIDDEN_DIM = 512\n",
        "LR = 1e-4\n",
        "EPOCHS = 3        # demo; có thể tăng thêm\n",
        "\n",
        "DEVICE = 'cuda' if __import__('torch').cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0f78116",
      "metadata": {
        "id": "d0f78116"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# IMPORTS & HÀM TIỆN ÍCH\n",
        "# =========================\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "try:\n",
        "    import nltk\n",
        "    from nltk.translate.bleu_score import corpus_bleu\n",
        "except ImportError:\n",
        "    nltk = None\n",
        "    print(\"Không tìm thấy nltk. BLEU sẽ không dùng được nếu chưa cài nltk.\")\n",
        "\n",
        "PAD, BOS, EOS, UNK = \"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"\n",
        "\n",
        "def tokenize(text):\n",
        "    return re.findall(r\"\\w+|\\S\", text.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb5f0f0",
      "metadata": {
        "id": "ddb5f0f0"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# XÂY DỰNG VOCAB & DATASET\n",
        "# =========================\n",
        "def build_vocab(caption_file, freq_min=5, max_size=10000):\n",
        "    counter = Counter()\n",
        "    with open(caption_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if '\\t' not in line:\n",
        "                continue\n",
        "            _, cap = line.strip().split('\\t')\n",
        "            toks = tokenize(cap)\n",
        "            counter.update(toks)\n",
        "    itos = [PAD, BOS, EOS, UNK]\n",
        "    for w, c in counter.most_common():\n",
        "        if c < freq_min:\n",
        "            break\n",
        "        if len(itos) >= max_size:\n",
        "            break\n",
        "        itos.append(w)\n",
        "    stoi = {w: i for i, w in enumerate(itos)}\n",
        "    print(\"Vocab size:\", len(itos))\n",
        "    return itos, stoi\n",
        "\n",
        "def encode_caption(cap, stoi, max_len=20):\n",
        "    toks = tokenize(cap)\n",
        "    ids = [stoi.get(t, stoi[UNK]) for t in toks]\n",
        "    ids = [stoi[BOS]] + ids + [stoi[EOS]]\n",
        "    if len(ids) < max_len:\n",
        "        ids = ids + [stoi[PAD]] * (max_len - len(ids))\n",
        "    return torch.tensor(ids[:max_len], dtype=torch.long)\n",
        "\n",
        "class Flickr8kDataset(Dataset):\n",
        "    def __init__(self, img_dir, caption_file, itos, stoi,\n",
        "                 max_len=20, transform=None, split='train', split_ratio=0.8):\n",
        "        self.img_dir = img_dir\n",
        "        self.itos, self.stoi = itos, stoi\n",
        "        self.max_len = max_len\n",
        "        self.transform = transform\n",
        "        self.samples = []  # list of (img_name, caption)\n",
        "\n",
        "        lines = []\n",
        "        with open(caption_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if '\\t' not in line:\n",
        "                    continue\n",
        "                lines.append(line.strip())\n",
        "        random.seed(42)\n",
        "        random.shuffle(lines)\n",
        "        n_train = int(len(lines) * split_ratio)\n",
        "        if split == 'train':\n",
        "            use_lines = lines[:n_train]\n",
        "        else:\n",
        "            use_lines = lines[n_train:]\n",
        "        for line in use_lines:\n",
        "            img_id, cap = line.split('\\t')\n",
        "            img_name = img_id.split('#')[0]\n",
        "            self.samples.append((img_name, cap))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, cap = self.samples[idx]\n",
        "        path = os.path.join(self.img_dir, img_name)\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        cap_ids = encode_caption(cap, self.stoi, self.max_len)\n",
        "        return img, cap_ids, img_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "263b0ad4",
      "metadata": {
        "id": "263b0ad4"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# TẠO DATALOADER\n",
        "# =========================\n",
        "# Transform cho CNN (và tạm thời cho mọi kiến trúc, trừ khi dùng ViT weights.transforms())\n",
        "transform_cnn = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "itos, stoi = build_vocab(CAPTION_FILE, freq_min=FREQ_MIN,\n",
        "                         max_size=VOCAB_MAX_SIZE)\n",
        "\n",
        "train_ds = Flickr8kDataset(IMG_DIR, CAPTION_FILE, itos, stoi,\n",
        "                           max_len=MAX_LEN, transform=transform_cnn,\n",
        "                           split='train', split_ratio=0.8)\n",
        "val_ds = Flickr8kDataset(IMG_DIR, CAPTION_FILE, itos, stoi,\n",
        "                         max_len=MAX_LEN, transform=transform_cnn,\n",
        "                         split='val', split_ratio=0.8)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
        "                          shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE,\n",
        "                        shuffle=False, num_workers=0)\n",
        "\n",
        "print(\"Train samples:\", len(train_ds), \"Val samples:\", len(val_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8410106",
      "metadata": {
        "id": "b8410106"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# NẠP GloVe (TÙY CHỌN CHO ARCH = 'cnn_lstm_attn')\n",
        "# =========================\n",
        "glove_vectors = None\n",
        "if ARCH == 'cnn_lstm_attn' and USE_GLOVE:\n",
        "    if not os.path.exists(GLOVE_PATH):\n",
        "        print(\"Không tìm thấy file GloVe, sẽ dùng embedding train từ đầu.\")\n",
        "        USE_GLOVE = False\n",
        "    else:\n",
        "        print(\"Đang đọc GloVe từ:\", GLOVE_PATH)\n",
        "        glove_vectors = {}\n",
        "        with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) < 51:\n",
        "                    continue\n",
        "                w = parts[0]\n",
        "                vec = np.asarray(parts[1:], dtype=np.float32)\n",
        "                glove_vectors[w] = vec\n",
        "        print(\"Số từ trong GloVe:\", len(glove_vectors))\n",
        "        EMB_DIM = 50  # cố định theo glove.6B.50d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71bb4a2",
      "metadata": {
        "id": "a71bb4a2"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# ENCODER: CNN & ViT\n",
        "# =========================\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, out_dim=512):\n",
        "        super().__init__()\n",
        "        m = torchvision.models.resnet50(\n",
        "            weights=torchvision.models.ResNet50_Weights.DEFAULT\n",
        "        )\n",
        "        self.cnn = nn.Sequential(*list(m.children())[:-2])  # B×2048×7×7\n",
        "        self.proj = nn.Conv2d(2048, out_dim, kernel_size=1)\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.cnn(x)               # B×2048×7×7\n",
        "        f = self.proj(f)              # B×D×7×7\n",
        "        B, D, H, W = f.shape\n",
        "        memory = f.flatten(2).permute(0, 2, 1)  # B×L×D, L=H*W\n",
        "        global_vec = f.mean(dim=[2, 3])         # B×D\n",
        "        return global_vec, memory, (H, W)\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, out_dim=512):\n",
        "        super().__init__()\n",
        "        weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "        m = torchvision.models.vit_b_16(weights=weights)\n",
        "        self.preprocess = weights.transforms()\n",
        "        self.vit = m\n",
        "        self.proj = nn.Linear(m.hidden_dim, out_dim)\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: B×3×H×W, cần resize + normalize theo weights\n",
        "        x = self.preprocess.transforms[0](x) if hasattr(self.preprocess, 'transforms') else x\n",
        "        # để đơn giản, giả sử đầu vào đã được chuẩn hóa giống transform_cnn\n",
        "        x = self.vit._process_input(x)\n",
        "        n = x.shape[0]\n",
        "        cls_token = self.vit.class_token.expand(n, -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        x = self.vit.encoder(x)           # B×(1+L)×Dh\n",
        "        x = self.proj(x)                  # B×(1+L)×D\n",
        "        cls = x[:, 0]                     # B×D\n",
        "        tokens = x[:, 1:]                 # B×L×D\n",
        "        return cls, tokens, None  # không có grid H×W rõ ràng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9b5034",
      "metadata": {
        "id": "4a9b5034"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# DECODER: LSTM + ATTENTION\n",
        "# =========================\n",
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, dim_q, dim_k, dim_h):\n",
        "        super().__init__()\n",
        "        self.Wq = nn.Linear(dim_q, dim_h)\n",
        "        self.Wk = nn.Linear(dim_k, dim_h)\n",
        "        self.v  = nn.Linear(dim_h, 1)\n",
        "\n",
        "    def forward(self, q, k, mask=None):\n",
        "        # q: B×H, k: B×L×D\n",
        "        B, L, D = k.shape\n",
        "        q_ = self.Wq(q).unsqueeze(1).expand(B, L, -1)\n",
        "        k_ = self.Wk(k)\n",
        "        e = self.v(torch.tanh(q_ + k_)).squeeze(-1)  # B×L\n",
        "        if mask is not None:\n",
        "            e = e.masked_fill(~mask.bool(), -1e9)\n",
        "        a = F.softmax(e, dim=-1)                     # B×L\n",
        "        c = torch.bmm(a.unsqueeze(1), k).squeeze(1)  # B×D\n",
        "        return c, a\n",
        "\n",
        "class LSTMAttnDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, mem_dim,\n",
        "                 padding_idx=0, pretrained_emb=None):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=padding_idx)\n",
        "        if pretrained_emb is not None:\n",
        "            self.emb.weight.data.copy_(torch.from_numpy(pretrained_emb))\n",
        "        self.lstm = nn.LSTM(emb_dim + mem_dim, hidden_dim,\n",
        "                            batch_first=True)\n",
        "        self.attn = AdditiveAttention(hidden_dim, mem_dim, dim_h=256)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, captions, memory):\n",
        "        B, T = captions.shape\n",
        "        emb = self.emb(captions)         # B×T×E\n",
        "        h = torch.zeros(1, B, self.hidden_dim, device=captions.device)\n",
        "        c = torch.zeros(1, B, self.hidden_dim, device=captions.device)\n",
        "        outputs = []\n",
        "        for t in range(T):\n",
        "            h_t = h[-1]                  # B×H\n",
        "            ctx, _ = self.attn(h_t, memory)\n",
        "            x_t = torch.cat([emb[:, t, :], ctx], dim=-1).unsqueeze(1)\n",
        "            o_t, (h, c) = self.lstm(x_t, (h, c))\n",
        "            logits_t = self.fc(o_t.squeeze(1))\n",
        "            outputs.append(logits_t.unsqueeze(1))\n",
        "        return torch.cat(outputs, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9266fb64",
      "metadata": {
        "id": "9266fb64"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# DECODER: TRANSFORMER (CÓ LẤY RA ATTENTION)\n",
        "# =========================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() *\n",
        "            (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        L = x.size(1)\n",
        "        return x + self.pe[:, :L, :]\n",
        "\n",
        "class CustomDecoderLayer(nn.TransformerDecoderLayer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask=None,\n",
        "                memory_mask=None,\n",
        "                tgt_key_padding_mask=None,\n",
        "                memory_key_padding_mask=None,\n",
        "                need_attn=False):\n",
        "        # copy từ PyTorch, thêm trả về attn cross nếu need_attn=True\n",
        "        x = tgt\n",
        "        if self.norm_first:\n",
        "            x2 = self.self_attn(\n",
        "                self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)\n",
        "            )[0]\n",
        "            x = x + self.dropout1(x2)\n",
        "            x2, attn_cross = self.multihead_attn(\n",
        "                self._mha_block(self.norm2(x), memory, memory_mask,\n",
        "                                memory_key_padding_mask)\n",
        "            )\n",
        "            x = x + self.dropout2(x2)\n",
        "            x2 = self.linear2(self.dropout(self.activation(self.linear1(self.norm3(x)))))\n",
        "            x = x + self.dropout3(x2)\n",
        "        else:\n",
        "            x2 = self.self_attn(\n",
        "                x, x, x,\n",
        "                attn_mask=tgt_mask,\n",
        "                key_padding_mask=tgt_key_padding_mask\n",
        "            )[0]\n",
        "            x = x + self.dropout1(x2)\n",
        "            x = self.norm1(x)\n",
        "            x2, attn_cross = self.multihead_attn(\n",
        "                x, memory, memory,\n",
        "                attn_mask=memory_mask,\n",
        "                key_padding_mask=memory_key_padding_mask\n",
        "            )\n",
        "            x = x + self.dropout2(x2)\n",
        "            x = self.norm2(x)\n",
        "            x2 = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
        "            x = x + self.dropout3(x2)\n",
        "            x = self.norm3(x)\n",
        "        if need_attn:\n",
        "            return x, attn_cross\n",
        "        return x, None\n",
        "\n",
        "class TransformerCaptionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size,\n",
        "                 d_model=512, nhead=8, num_layers=6,\n",
        "                 padding_idx=0):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=padding_idx)\n",
        "        self.pos = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([\n",
        "            CustomDecoderLayer(d_model, nhead,\n",
        "                               dim_feedforward=1024,\n",
        "                               batch_first=True)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, captions_in, memory,\n",
        "                tgt_padding_mask=None, mem_padding_mask=None,\n",
        "                return_attn=False):\n",
        "        # captions_in: B×T_in (BOS..)\n",
        "        x = self.emb(captions_in) * math.sqrt(self.d_model)\n",
        "        x = self.pos(x)\n",
        "        T = x.size(1)\n",
        "        causal = torch.triu(\n",
        "            torch.ones(T, T, device=x.device), 1\n",
        "        ).bool()\n",
        "        attn_last = None\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x, attn = layer(\n",
        "                x, memory,\n",
        "                tgt_mask=causal,\n",
        "                memory_mask=None,\n",
        "                tgt_key_padding_mask=tgt_padding_mask,\n",
        "                memory_key_padding_mask=mem_padding_mask,\n",
        "                need_attn=(return_attn and i == len(self.layers)-1)\n",
        "            )\n",
        "            if attn is not None:\n",
        "                # attn: B×num_heads×T×L\n",
        "                attn_last = attn\n",
        "        x = self.norm(x)\n",
        "        logits = self.fc(x)  # B×T×V\n",
        "        if return_attn:\n",
        "            return logits, attn_last\n",
        "        return logits, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a11cac4",
      "metadata": {
        "id": "3a11cac4"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# GNN NHẸ (Simple GCN)\n",
        "# =========================\n",
        "class SimpleGCN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # x: B×N×D, adj: B×N×N (0/1)\n",
        "        deg = adj.sum(-1, keepdim=True) + 1e-6\n",
        "        A_norm = adj / deg\n",
        "        h = torch.bmm(A_norm, self.lin1(x))  # B×N×H\n",
        "        h = F.relu(h)\n",
        "        h = torch.bmm(A_norm, self.lin2(h))  # B×N×H\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f61f0b4",
      "metadata": {
        "id": "3f61f0b4"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# KHỞI TẠO MÔ HÌNH\n",
        "# =========================\n",
        "vocab_size = len(itos)\n",
        "\n",
        "encoder = None\n",
        "decoder = None\n",
        "gcn = None\n",
        "\n",
        "if ARCH in ['cnn_lstm_attn', 'cnn_tf']:\n",
        "    encoder = CNNEncoder(out_dim=512).to(DEVICE)\n",
        "elif ARCH in ['vit_tf', 'vit_tf_gnn']:\n",
        "    encoder = ViTEncoder(out_dim=512).to(DEVICE)\n",
        "else:\n",
        "    raise ValueError(\"ARCH không hợp lệ\")\n",
        "\n",
        "# Chuẩn bị embedding GloVe (nếu dùng LSTM + Attention)\n",
        "pretrained_emb = None\n",
        "if ARCH == 'cnn_lstm_attn':\n",
        "    if USE_GLOVE and glove_vectors is not None:\n",
        "        emb_matrix = np.random.normal(scale=0.1, size=(vocab_size, EMB_DIM)).astype(np.float32)\n",
        "        for i, w in enumerate(itos):\n",
        "            if w in glove_vectors:\n",
        "                emb_matrix[i] = glove_vectors[w]\n",
        "        pretrained_emb = emb_matrix\n",
        "        print(\"Khởi tạo embedding từ GloVe với shape:\", emb_matrix.shape)\n",
        "    decoder = LSTMAttnDecoder(vocab_size, EMB_DIM, HIDDEN_DIM, mem_dim=512,\n",
        "                              padding_idx=stoi[PAD],\n",
        "                              pretrained_emb=pretrained_emb).to(DEVICE)\n",
        "else:\n",
        "    # Transformer decoder\n",
        "    decoder = TransformerCaptionDecoder(vocab_size, d_model=512,\n",
        "                                        nhead=8, num_layers=4,\n",
        "                                        padding_idx=stoi[PAD]).to(DEVICE)\n",
        "\n",
        "# GNN cho ARCH vit_tf_gnn (demo: sinh features object random)\n",
        "if ARCH == 'vit_tf_gnn':\n",
        "    gcn = SimpleGCN(in_dim=256, hidden_dim=256).to(DEVICE)\n",
        "\n",
        "# Optimizer & loss\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=stoi[PAD])\n",
        "params = list(decoder.parameters())\n",
        "# encoder ta có thể fine-tune nhẹ hoặc freeze; để đơn giản: freeze CNN\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "optimizer = torch.optim.Adam(params, lr=LR)\n",
        "\n",
        "print(\"Model ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a10b79",
      "metadata": {
        "id": "95a10b79"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# HÀM TRAIN & EVAL\n",
        "# =========================\n",
        "def forward_batch(images, captions, encoder, decoder, arch,\n",
        "                  gcn=None):\n",
        "    images = images.to(DEVICE)\n",
        "    captions = captions.to(DEVICE)\n",
        "    bos_id = stoi[BOS]\n",
        "\n",
        "    if arch in ['cnn_lstm_attn', 'cnn_tf']:\n",
        "        global_vec, memory, grid = encoder(images)\n",
        "    else:\n",
        "        global_vec, memory, grid = encoder(images)\n",
        "\n",
        "    if arch == 'vit_tf_gnn' and gcn is not None:\n",
        "        # DEMO: tạo object features & adjacency random\n",
        "        B = images.size(0)\n",
        "        N = 5\n",
        "        obj_feats = torch.randn(B, N, 256, device=images.device)\n",
        "        adj = torch.ones(B, N, N, device=images.device)\n",
        "        x_gnn = gcn(obj_feats, adj)  # B×N×256\n",
        "        graph_vec = x_gnn.mean(dim=1)  # B×256\n",
        "        # fuse vào memory bằng cách cộng bias\n",
        "        bias = graph_vec.unsqueeze(1)  # B×1×256\n",
        "        if memory.size(-1) == bias.size(-1):\n",
        "            memory = memory + bias\n",
        "\n",
        "    inp = captions[:, :-1]\n",
        "    tgt = captions[:, 1:]\n",
        "\n",
        "    if arch == 'cnn_lstm_attn':\n",
        "        logits = decoder(inp, memory)  # B×T×V\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)),\n",
        "                         tgt.reshape(-1))\n",
        "        return loss, None\n",
        "    else:\n",
        "        logits, _ = decoder(inp, memory)\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)),\n",
        "                         tgt.reshape(-1))\n",
        "        return loss, None\n",
        "\n",
        "\n",
        "def train_one_epoch(loader, encoder, decoder, arch, optimizer, gcn=None):\n",
        "    decoder.train()\n",
        "    total_loss = 0.0\n",
        "    for images, caps, _ in loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss, _ = forward_batch(images, caps, encoder, decoder, arch, gcn)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "def evaluate_bleu(loader, encoder, decoder, arch, gcn=None, max_samples=200):\n",
        "    if nltk is None:\n",
        "        print(\"Chưa có nltk, không tính BLEU được.\")\n",
        "        return None\n",
        "    decoder.eval()\n",
        "    all_refs = []\n",
        "    all_hyps = []\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for images, caps, _ in loader:\n",
        "            for i in range(images.size(0)):\n",
        "                if cnt >= max_samples:\n",
        "                    break\n",
        "                img = images[i:i+1].to(DEVICE)\n",
        "                ref = caps[i].tolist()\n",
        "                hyp_tokens = greedy_decode(encoder, decoder, arch, img, gcn=gcn)\n",
        "                # ref: convert to tokens without pad/bos\n",
        "                ref_tokens = []\n",
        "                for idx in ref:\n",
        "                    if idx == stoi[PAD] or idx == stoi[BOS]:\n",
        "                        continue\n",
        "                    if idx == stoi[EOS]:\n",
        "                        break\n",
        "                    ref_tokens.append(itos[idx])\n",
        "                all_refs.append([ref_tokens])\n",
        "                all_hyps.append(hyp_tokens)\n",
        "                cnt += 1\n",
        "            if cnt >= max_samples:\n",
        "                break\n",
        "    bleu4 = corpus_bleu(all_refs, all_hyps, weights=(0.25,0.25,0.25,0.25))\n",
        "    print(\"BLEU-4:\", bleu4)\n",
        "    return bleu4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "828bb82f",
      "metadata": {
        "id": "828bb82f"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# HÀM GREEDY DECODE + LẤY ATTENTION\n",
        "# =========================\n",
        "def greedy_decode(encoder, decoder, arch, image_tensor,\n",
        "                  max_len=20, gcn=None):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        img = image_tensor.to(DEVICE)\n",
        "        if img.ndim == 3:\n",
        "            img = img.unsqueeze(0)\n",
        "        global_vec, memory, grid = encoder(img)\n",
        "\n",
        "        if arch == 'vit_tf_gnn' and gcn is not None:\n",
        "            B = img.size(0)\n",
        "            N = 5\n",
        "            obj_feats = torch.randn(B, N, 256, device=img.device)\n",
        "            adj = torch.ones(B, N, N, device=img.device)\n",
        "            x_gnn = gcn(obj_feats, adj)\n",
        "            graph_vec = x_gnn.mean(dim=1)\n",
        "            bias = graph_vec.unsqueeze(1)\n",
        "            if memory.size(-1) == bias.size(-1):\n",
        "                memory = memory + bias\n",
        "\n",
        "        bos_id = stoi[BOS]\n",
        "        eos_id = stoi[EOS]\n",
        "        ids = [bos_id]\n",
        "\n",
        "        if arch == 'cnn_lstm_attn':\n",
        "            h = torch.zeros(1, 1, decoder.lstm.hidden_dim, device=img.device)\n",
        "            c = torch.zeros(1, 1, decoder.lstm.hidden_dim, device=img.device)\n",
        "            tokens = []\n",
        "            for t in range(max_len):\n",
        "                inp = torch.tensor([[ids[-1]]], device=img.device)\n",
        "                emb = decoder.emb(inp)\n",
        "                h_t = h[-1]\n",
        "                ctx, _ = decoder.attn(h_t, memory)\n",
        "                x_t = torch.cat([emb.squeeze(1), ctx], dim=-1).unsqueeze(1)\n",
        "                o, (h, c) = decoder.lstm(x_t, (h, c))\n",
        "                logits = decoder.fc(o.squeeze(1))\n",
        "                next_id = logits.argmax(-1).item()\n",
        "                if next_id == eos_id:\n",
        "                    break\n",
        "                ids.append(next_id)\n",
        "                tokens.append(itos[next_id])\n",
        "            return tokens\n",
        "        else:\n",
        "            # Transformer decoder\n",
        "            tokens = []\n",
        "            inp = torch.tensor([[bos_id]], device=img.device)\n",
        "            for t in range(max_len):\n",
        "                logits, _ = decoder(inp, memory, return_attn=False)\n",
        "                next_id = logits[0, -1].argmax(-1).item()\n",
        "                if next_id == eos_id:\n",
        "                    break\n",
        "                inp = torch.cat([inp, torch.tensor([[next_id]], device=img.device)], dim=1)\n",
        "                tokens.append(itos[next_id])\n",
        "            return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61383970",
      "metadata": {
        "id": "61383970"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# VIZ ATTENTION (CNN + LSTM + ATTENTION)\n",
        "# =========================\n",
        "def generate_with_attn(encoder, decoder, image_tensor,\n",
        "                       stoi, itos, max_len=20, device=\"cuda\"):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        img = image_tensor.unsqueeze(0).to(device)\n",
        "        global_vec, memory, grid = encoder(img)      # memory: 1×L×D\n",
        "        H, W = grid if grid is not None else (7, 7)\n",
        "\n",
        "        bos_id = stoi[BOS]\n",
        "        eos_id = stoi[EOS]\n",
        "        ids = [bos_id]\n",
        "        tokens = []\n",
        "        attn_maps = []    # list of (L,) numpy\n",
        "\n",
        "        h = torch.zeros(1, 1, decoder.lstm.hidden_dim, device=device)\n",
        "        c = torch.zeros(1, 1, decoder.lstm.hidden_dim, device=device)\n",
        "\n",
        "        for t in range(max_len):\n",
        "            inp = torch.tensor([[ids[-1]]], device=device)\n",
        "            emb = decoder.emb(inp)\n",
        "            h_t = h[-1]                      # 1×H\n",
        "            ctx, att = decoder.attn(h_t, memory)  # att: 1×L\n",
        "            attn_maps.append(att.squeeze(0).cpu().numpy())  # L\n",
        "\n",
        "            x_t = torch.cat(\n",
        "                [emb.squeeze(1), ctx], dim=-1\n",
        "            ).unsqueeze(1)\n",
        "            o, (h, c) = decoder.lstm(x_t, (h, c))\n",
        "            logits = decoder.fc(o.squeeze(1))\n",
        "            next_id = logits.argmax(-1).item()\n",
        "\n",
        "            if next_id == eos_id:\n",
        "                break\n",
        "            ids.append(next_id)\n",
        "            tokens.append(itos[next_id])\n",
        "\n",
        "        return \" \".join(tokens), tokens, attn_maps, (H, W)\n",
        "\n",
        "\n",
        "def show_attention_on_image(img_pil, tokens, attn_maps,\n",
        "                            H=7, W=7, cols=4):\n",
        "    import math\n",
        "    import numpy as np\n",
        "    from PIL import Image as PILImage\n",
        "\n",
        "    n = len(tokens)\n",
        "    rows = math.ceil(n / cols)\n",
        "    fig = plt.figure(figsize=(3*cols, 3*rows))\n",
        "\n",
        "    for i, (word, attn_vec) in enumerate(zip(tokens, attn_maps)):\n",
        "        ax = fig.add_subplot(rows, cols, i + 1)\n",
        "        ax.set_title(word)\n",
        "\n",
        "        attn = attn_vec.reshape(H, W)\n",
        "        attn = attn / (attn.max() + 1e-8)\n",
        "        attn = np.clip(attn, 0.0, 1.0)\n",
        "\n",
        "        attn_img = PILImage.fromarray(\n",
        "            (attn * 255).astype(np.uint8)\n",
        "        ).resize(img_pil.size, resample=PILImage.BILINEAR)\n",
        "\n",
        "        ax.imshow(img_pil)\n",
        "        ax.imshow(attn_img, cmap=\"jet\", alpha=0.4)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def attn_to_mask(attn_vec, H=7, W=7, threshold=0.6,\n",
        "                 use_percentile=True):\n",
        "    import numpy as np\n",
        "    attn = attn_vec.reshape(H, W)\n",
        "    attn = attn / (attn.max() + 1e-8)\n",
        "    if use_percentile:\n",
        "        thr_value = np.quantile(attn, threshold)\n",
        "    else:\n",
        "        thr_value = threshold\n",
        "    mask = (attn >= thr_value).astype(np.float32)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def show_attention_mask_on_image(img_pil, tokens, attn_maps,\n",
        "                                 H=7, W=7, cols=4,\n",
        "                                 threshold=0.6,\n",
        "                                 use_percentile=True):\n",
        "    import math\n",
        "    import numpy as np\n",
        "    from PIL import Image as PILImage\n",
        "\n",
        "    n = len(tokens)\n",
        "    rows = math.ceil(n / cols)\n",
        "    fig = plt.figure(figsize=(3*cols, 3*rows))\n",
        "\n",
        "    img_np = np.array(img_pil).astype(np.float32) / 255.0\n",
        "\n",
        "    for i, (word, attn_vec) in enumerate(zip(tokens, attn_maps)):\n",
        "        ax = fig.add_subplot(rows, cols, i + 1)\n",
        "        ax.set_title(word)\n",
        "\n",
        "        mask_hw = attn_to_mask(attn_vec, H, W,\n",
        "                               threshold=threshold,\n",
        "                               use_percentile=use_percentile)\n",
        "\n",
        "        mask_img = PILImage.fromarray(\n",
        "            (mask_hw * 255).astype(np.uint8)\n",
        "        ).resize(img_pil.size, resample=PILImage.NEAREST)\n",
        "        mask_np = np.array(mask_img).astype(np.float32) / 255.0\n",
        "        mask_np = mask_np[..., None]\n",
        "\n",
        "        bg = img_np * 0.3\n",
        "        red_overlay = np.zeros_like(img_np)\n",
        "        red_overlay[..., 0] = 1.0\n",
        "\n",
        "        out = bg * (1 - mask_np) +               (img_np * 0.7 + red_overlay * 0.3) * mask_np\n",
        "\n",
        "        ax.imshow(out)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8faff59",
      "metadata": {
        "id": "a8faff59"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# DEMO TRAIN NHANH 1 EPOCH + EVAL BLEU (OPTIONAL)\n",
        "# =========================\n",
        "for epoch in range(EPOCHS):\n",
        "    loss = train_one_epoch(train_loader, encoder, decoder,\n",
        "                           ARCH, optimizer, gcn=gcn)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, loss = {loss:.4f}\")\n",
        "    evaluate_bleu(val_loader, encoder, decoder, ARCH, gcn=gcn, max_samples=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8f5c028",
      "metadata": {
        "id": "c8f5c028"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# DEMO INFERENCE TRÊN 1 ẢNH + VIZ ATTENTION (CHO ARCH = cnn_lstm_attn)\n",
        "# =========================\n",
        "test_img_path = None  # điền đường dẫn 1 ảnh cụ thể nếu muốn\n",
        "\n",
        "if test_img_path is not None and ARCH == 'cnn_lstm_attn':\n",
        "    img_pil = Image.open(test_img_path).convert('RGB')\n",
        "    img_t = transform_cnn(img_pil)\n",
        "    caption, tokens, attn_maps, (H, W) = generate_with_attn(\n",
        "        encoder, decoder, img_t, stoi, itos, device=DEVICE\n",
        "    )\n",
        "    print(\"Caption:\", caption)\n",
        "    show_attention_on_image(img_pil, tokens, attn_maps, H, W)\n",
        "    show_attention_mask_on_image(img_pil, tokens, attn_maps, H, W,\n",
        "                                 threshold=0.6, use_percentile=True)\n",
        "else:\n",
        "    print(\"Đặt test_img_path và ARCH='cnn_lstm_attn' để viz attention.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec28ba76",
      "metadata": {
        "id": "ec28ba76"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# DEMO INFERENCE TRÊN CẢ THƯ MỤC ẢNH\n",
        "# =========================\n",
        "def run_inference_on_folder(img_dir, encoder, decoder,\n",
        "                            arch, stoi, itos,\n",
        "                            transform, gcn=None,\n",
        "                            max_images=10):\n",
        "    files = [f for f in os.listdir(img_dir)\n",
        "             if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
        "    files = files[:max_images]\n",
        "    for fname in files:\n",
        "        path = os.path.join(img_dir, fname)\n",
        "        img_pil = Image.open(path).convert('RGB')\n",
        "        img_t = transform(img_pil)\n",
        "        tokens = greedy_decode(encoder, decoder, arch,\n",
        "                               img_t.unsqueeze(0), gcn=gcn)\n",
        "        caption = \" \".join(tokens)\n",
        "        plt.figure(figsize=(4,4))\n",
        "        plt.imshow(img_pil)\n",
        "        plt.axis('off')\n",
        "        plt.title(caption)\n",
        "        plt.show()\n",
        "\n",
        "# Ví dụ chạy:\n",
        "# run_inference_on_folder(IMG_DIR, encoder, decoder,\n",
        "#                         ARCH, stoi, itos,\n",
        "#                         transform_cnn, gcn=gcn, max_images=5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}