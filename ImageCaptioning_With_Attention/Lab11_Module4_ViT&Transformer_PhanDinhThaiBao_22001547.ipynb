{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.nn.utils.rnn import pad_sequence\nimport collections\nfrom torch.utils.data import random_split\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Cấu hình thiết bị\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --- 1. Xây dựng Vocabulary ---\nclass Vocabulary:\n    def __init__(self, freq_threshold):\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.freq_threshold = freq_threshold\n\n    def __len__(self):\n        return len(self.itos)\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = collections.Counter()\n        idx = 4\n        \n        for sentence in sentence_list:\n            for word in sentence.lower().split():\n                frequencies[word] += 1\n                \n        for word, count in frequencies.items():\n            if count >= self.freq_threshold:\n                self.stoi[word] = idx\n                self.itos[idx] = word\n                idx += 1\n\n    def numericalize(self, text):\n        tokenized_text = text.lower().split()\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n            for token in tokenized_text\n        ]\n\n# --- 2. Xây dựng Dataset ---\nclass Flickr8kDataset(Dataset):\n    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(captions_file)\n        self.transform = transform\n        \n        # Lấy captions và ảnh\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n        \n        # Xây dựng vocab\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocabulary(self.captions.tolist())\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        caption = self.captions[index]\n        img_id = self.imgs[index]\n        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n\n        return img, torch.tensor(numericalized_caption)\n\n# --- 3. Collate Function (Padding) ---\nclass MyCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs, dim=0)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n        return imgs, targets\n\n# --- Thiết lập Path và Loader ---\n# Lưu ý: Cấu trúc thư mục trên Kaggle thường là /kaggle/input/flickr8k/Images và captions.txt\nimage_folder = '/kaggle/input/flickr8k/Images'\ncaptions_file = '/kaggle/input/flickr8k/captions.txt'\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)), # ResNet chuẩn input 224\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\n# Load dữ liệu (Cần đảm bảo file tồn tại, nếu không code sẽ báo lỗi)\ntry:\n    dataset = Flickr8kDataset(image_folder, captions_file, transform=transform)\n    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n    # train_loader = DataLoader(\n    #     dataset=dataset,\n    #     batch_size=32, # Batch size theo yêu cầu\n    #     num_workers=2,\n    #     shuffle=True,\n    #     collate_fn=MyCollate(pad_idx=pad_idx)\n    # )\n\n    # 1. Xác định kích thước tập train và test (ví dụ: 90% train, 10% test)\n    train_size = int(0.9 * len(dataset))\n    test_size = len(dataset) - train_size\n    \n    # 2. Chia ngẫu nhiên dataset\n    train_set, test_set = random_split(dataset, [train_size, test_size])\n    \n    # 3. Tạo lại Train Loader (nếu muốn train trên đúng tập train_set mới)\n    train_loader = DataLoader(\n        dataset=train_set,\n        batch_size=32,\n        num_workers=2,\n        shuffle=True,\n        collate_fn=MyCollate(pad_idx=pad_idx)\n    )\n    \n    # 4. Tạo Test Loader (đây là biến bạn đang thiếu)\n    test_loader = DataLoader(\n        dataset=test_set,\n        batch_size=32,\n        num_workers=2,\n        shuffle=False, # Test không cần shuffle\n        collate_fn=MyCollate(pad_idx=pad_idx)\n    )\n\n    print(f\"Train size: {len(train_set)}, Test size: {len(test_set)}\")\n    print(f\"Vocab size: {len(dataset.vocab)}\")\nexcept Exception as e:\n    print(f\"Lỗi load data: {e}. Hãy kiểm tra lại đường dẫn file trên Kaggle.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T15:40:02.361974Z","iopub.execute_input":"2025-12-09T15:40:02.363010Z","iopub.status.idle":"2025-12-09T15:40:02.592763Z","shell.execute_reply.started":"2025-12-09T15:40:02.362964Z","shell.execute_reply":"2025-12-09T15:40:02.592132Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTrain size: 36409, Test size: 4046\nVocab size: 3005\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import torchvision.models as models\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.models import ViT_B_16_Weights\n\nclass ViTEncoder(nn.Module):\n    def __init__(self, out_dim=512):\n        super().__init__()\n        # Load pre-trained ViT-B/16\n        weights = ViT_B_16_Weights.DEFAULT\n        self.vit = torchvision.models.vit_b_16(weights=weights)\n        \n        # ViT-B/16 có hidden_dim = 768. \n        # Ta cần chiếu về out_dim (512) để khớp với Decoder LSTM ở Module 1\n        self.proj = nn.Linear(768, out_dim)\n        \n        # Freeze các tầng đầu của ViT để train nhanh hơn (tùy chọn)\n        for param in self.vit.parameters():\n            param.requires_grad = False \n        # Unfreeze projection layer\n        for param in self.proj.parameters():\n            param.requires_grad = True\n\n    def forward(self, x):\n        # x: B x 3 x 224 x 224\n        \n        # 1. Chuyển ảnh thành patch embeddings\n        # Hàm _process_input thực hiện: Conv2d (chia patch) -> Flatten -> Transpose\n        x = self.vit._process_input(x) \n        n = x.shape[0]\n\n        # 2. Thêm Class Token (Learnable)\n        batch_class_token = self.vit.class_token.expand(n, -1, -1)\n        x = torch.cat([batch_class_token, x], dim=1) # B x (1 + 196) x 768 (với ảnh 224x224, patch 16 -> 14x14=196 patches)\n\n        # 3. Cộng Positional Embedding và đi qua Encoder Layers\n        x = self.vit.encoder(x) # B x 197 x 768\n\n        # 4. Chiếu về chiều dữ liệu mong muốn\n        x = self.proj(x) # B x 197 x 512\n\n        # Tách CLS token và Patch tokens\n        cls_token = x[:, 0]     # Global Vector (B x 512)\n        patch_tokens = x[:, 1:] # Memory cho Attention (B x 196 x 512)\n        \n        return cls_token, patch_tokens\n\n# --- 1. Positional Encoding ---\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        # Tạo ma trận PE\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1).float()\n        \n        # Công thức: div_term = 10000^(2i/d_model)\n        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        \n        # Thêm batch dimension: 1 x max_len x D\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        # x: B x L x D\n        L = x.size(1)\n        # Cộng PE vào embedding, slice theo độ dài thực tế\n        return x + self.pe[:, :L, :]\n\n# --- 2. Transformer Decoder ---\nclass TransformerCaptionDecoder(nn.Module):\n    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, max_len=100):\n        super().__init__()\n        \n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model, max_len=max_len)\n        \n        # Decoder Layer chuẩn của PyTorch\n        # batch_first=True giúp input/output có dạng [Batch, Seq, Dim]\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=d_model, \n            nhead=nhead, \n            dim_feedforward=2048, # Thường gấp 4 lần d_model\n            dropout=0.1,\n            batch_first=True \n        )\n        \n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(d_model, vocab_size)\n        self.d_model = d_model\n\n    def generate_square_subsequent_mask(self, sz, device):\n        # Tạo mask che phần tương lai (triangular mask)\n        # Giá trị -inf để softmax về 0, hoặc True/False tuỳ version\n        mask = torch.triu(torch.ones(sz, sz, device=device) * float('-inf'), diagonal=1)\n        return mask\n\n    def forward(self, captions, memory, tgt_padding_mask=None):\n        # captions: B x T (Target input)\n        # memory: B x L x D (Image features from CNN)\n        \n        # 1. Embedding + Positional Encoding\n        tgt = self.emb(captions) * math.sqrt(self.d_model) # Scale embedding\n        tgt = self.pos(tgt)\n        \n        # 2. Tạo Causal Mask (Che tương lai)\n        T = tgt.size(1)\n        tgt_mask = self.generate_square_subsequent_mask(T, tgt.device)\n        \n        # 3. Transformer Decoder Pass\n        # memory_key_padding_mask=None vì ảnh có kích thước cố định (49 vị trí)\n        output = self.decoder(\n            tgt=tgt, \n            memory=memory, \n            tgt_mask=tgt_mask,\n            tgt_key_padding_mask=tgt_padding_mask \n        )\n        \n        # 4. Dự đoán từ\n        logits = self.fc(output) # B x T x Vocab\n        return logits\n        \n\nclass TransformerImageCaptioning(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        \n    def forward(self, images, captions):\n        _, memory = self.encoder(images) # Memory: B x 49 x 512\n        tgt_padding_mask = (captions == 0) # Giả sử <PAD> index là 0\n        logits = self.decoder(captions, memory, tgt_padding_mask=tgt_padding_mask)\n        \n        return logits\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T15:40:02.593930Z","iopub.execute_input":"2025-12-09T15:40:02.594146Z","iopub.status.idle":"2025-12-09T15:40:02.608722Z","shell.execute_reply.started":"2025-12-09T15:40:02.594129Z","shell.execute_reply":"2025-12-09T15:40:02.608142Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# --- Cấu hình Hyperparameters ---\n# vocab_size lấy từ dataset Module 1\nd_model = 512 \nnhead = 8\nnum_layers = 4 \n\n# Khởi tạo\nencoder = CNNEncoder(out_dim=d_model).to(device) # Sử dụng CNNEncoder từ Module 1\ndecoder = TransformerCaptionDecoder(vocab_size, d_model, nhead, num_layers).to(device)\nmodel = ViTLSTMVgModel(encoder, decoder).to(device)\n\nimport torch.optim as optim\n\n# --- Training Configuration ---\ncriterion = nn.CrossEntropyLoss(ignore_index=0) # 0 is padding idx\n\n# Learning rate phân tầng (Differential Learning Rate)\n# ViT đã pre-train nên để LR rất nhỏ, Decoder train từ đầu nên để LR cao hơn\noptimizer = optim.Adam([\n    {'params': model.encoder.parameters(), 'lr': 1e-5},\n    {'params': model.decoder.parameters(), 'lr': 1e-4}\n])\n\n# --- Training Loop ---\nprint(\"Bắt đầu huấn luyện ViT + Transformer Decoder...\")\nmodel.train()\n\nnum_epochs = 3 \nfor epoch in range(num_epochs):\n    epoch_loss = 0\n    for idx, (imgs, captions) in enumerate(train_loader):\n        imgs = imgs.to(device)\n        captions = captions.to(device)\n        \n        # Decoder input: bỏ token cuối\n        decoder_input = captions[:, :-1]\n        # Target: bỏ token đầu <SOS>\n        targets = captions[:, 1:]\n        \n        # Forward pass\n        # Transformer tự xử lý masking bên trong\n        outputs = model(imgs, decoder_input)\n        \n        # Flatten and Loss\n        loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n        \n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient Clipping (Rất quan trọng với Transformer để tránh nan loss)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        epoch_loss += loss.item()\n        \n        if idx % 50 == 0:\n            print(f\"Epoch {epoch+1}, Step {idx}, Loss: {loss.item():.4f}\")\n            \n    print(f\"End Epoch {epoch+1}, Avg Loss: {epoch_loss/len(train_loader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T15:40:02.610191Z","iopub.execute_input":"2025-12-09T15:40:02.610404Z","iopub.status.idle":"2025-12-09T15:51:47.587966Z","shell.execute_reply.started":"2025-12-09T15:40:02.610390Z","shell.execute_reply":"2025-12-09T15:51:47.587260Z"}},"outputs":[{"name":"stdout","text":"Bắt đầu huấn luyện ViT + Transformer Decoder...\nEpoch 1, Step 0, Loss: 8.1995\nEpoch 1, Step 50, Loss: 4.5011\nEpoch 1, Step 100, Loss: 4.2636\nEpoch 1, Step 150, Loss: 3.8227\nEpoch 1, Step 200, Loss: 3.8478\nEpoch 1, Step 250, Loss: 3.5585\nEpoch 1, Step 300, Loss: 3.5896\nEpoch 1, Step 350, Loss: 3.5196\nEpoch 1, Step 400, Loss: 3.3648\nEpoch 1, Step 450, Loss: 3.2477\nEpoch 1, Step 500, Loss: 3.2910\nEpoch 1, Step 550, Loss: 3.2742\nEpoch 1, Step 600, Loss: 3.4358\nEpoch 1, Step 650, Loss: 3.2104\nEpoch 1, Step 700, Loss: 3.3444\nEpoch 1, Step 750, Loss: 3.1201\nEpoch 1, Step 800, Loss: 3.0014\nEpoch 1, Step 850, Loss: 2.7298\nEpoch 1, Step 900, Loss: 3.0843\nEpoch 1, Step 950, Loss: 2.6566\nEpoch 1, Step 1000, Loss: 2.9127\nEpoch 1, Step 1050, Loss: 3.0958\nEpoch 1, Step 1100, Loss: 2.8712\nEnd Epoch 1, Avg Loss: 3.4323\nEpoch 2, Step 0, Loss: 2.8865\nEpoch 2, Step 50, Loss: 2.8317\nEpoch 2, Step 100, Loss: 2.7789\nEpoch 2, Step 150, Loss: 2.9493\nEpoch 2, Step 200, Loss: 2.8489\nEpoch 2, Step 250, Loss: 2.9348\nEpoch 2, Step 300, Loss: 2.8334\nEpoch 2, Step 350, Loss: 2.6431\nEpoch 2, Step 400, Loss: 2.8995\nEpoch 2, Step 450, Loss: 2.7419\nEpoch 2, Step 500, Loss: 2.7797\nEpoch 2, Step 550, Loss: 2.7075\nEpoch 2, Step 600, Loss: 2.9108\nEpoch 2, Step 650, Loss: 2.8185\nEpoch 2, Step 700, Loss: 2.7306\nEpoch 2, Step 750, Loss: 2.5964\nEpoch 2, Step 800, Loss: 2.5152\nEpoch 2, Step 850, Loss: 2.5280\nEpoch 2, Step 900, Loss: 2.3771\nEpoch 2, Step 950, Loss: 2.4891\nEpoch 2, Step 1000, Loss: 2.3776\nEpoch 2, Step 1050, Loss: 2.6252\nEpoch 2, Step 1100, Loss: 2.6809\nEnd Epoch 2, Avg Loss: 2.6806\nEpoch 3, Step 0, Loss: 2.2792\nEpoch 3, Step 50, Loss: 2.4332\nEpoch 3, Step 100, Loss: 2.5910\nEpoch 3, Step 150, Loss: 2.5014\nEpoch 3, Step 200, Loss: 2.4140\nEpoch 3, Step 250, Loss: 2.5998\nEpoch 3, Step 300, Loss: 2.4722\nEpoch 3, Step 350, Loss: 2.1816\nEpoch 3, Step 400, Loss: 2.4716\nEpoch 3, Step 450, Loss: 2.0446\nEpoch 3, Step 500, Loss: 2.2400\nEpoch 3, Step 550, Loss: 2.4335\nEpoch 3, Step 600, Loss: 2.2088\nEpoch 3, Step 650, Loss: 2.2134\nEpoch 3, Step 700, Loss: 2.0580\nEpoch 3, Step 750, Loss: 2.4777\nEpoch 3, Step 800, Loss: 2.1263\nEpoch 3, Step 850, Loss: 2.3928\nEpoch 3, Step 900, Loss: 2.3807\nEpoch 3, Step 950, Loss: 2.0951\nEpoch 3, Step 1000, Loss: 2.4447\nEpoch 3, Step 1050, Loss: 2.4045\nEpoch 3, Step 1100, Loss: 2.4046\nEnd Epoch 3, Avg Loss: 2.3751\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# from torchtext.data.metrics import bleu_score\n# Hoặc dùng nltk nếu torchtext bản mới đổi API:\nfrom nltk.translate.bleu_score import corpus_bleu\n\ndef evaluate_bleu(model, dataloader, device, vocab):\n    model.eval()\n    refs = []  # Danh sách caption gốc\n    hyps = []  # Danh sách caption dự đoán\n    \n    with torch.no_grad():\n        for idx, (imgs, captions) in enumerate(dataloader):\n            if idx > 20: break # Test nhanh trên 20 batch đầu\n            imgs = imgs.to(device)\n            \n            # Encoder\n            _, memory = model.encoder(imgs)\n            \n            # Autoregressive Generation\n            # Bắt đầu với <SOS>\n            batch_size = imgs.size(0)\n            ys = torch.ones(batch_size, 1).fill_(vocab.stoi[\"<SOS>\"]).type(torch.long).to(device)\n            \n            for i in range(20): # Max length caption\n                tgt_mask = model.decoder.generate_square_subsequent_mask(ys.size(1), device)\n                \n                # Gọi decoder với chuỗi hiện tại ys\n                out = model.decoder.decoder(\n                    model.decoder.pos(model.decoder.emb(ys) * math.sqrt(d_model)), \n                    memory, \n                    tgt_mask=tgt_mask\n                )\n                \n                # Lấy logit của từ cuối cùng\n                prob = model.decoder.fc(out[:, -1])\n                _, next_word = torch.max(prob, dim=1)\n                \n                ys = torch.cat([ys, next_word.unsqueeze(1)], dim=1)\n            \n            # Convert IDs to Words\n            for j in range(batch_size):\n                # Xử lý caption gốc\n                ref_tokens = [vocab.itos[t.item()] for t in captions[j] if t.item() not in [0, 1, 2]]\n                refs.append([ref_tokens]) # NLTK cần list of lists cho refs\n                \n                # Xử lý caption dự đoán\n                pred_tokens = [vocab.itos[t.item()] for t in ys[j] if t.item() not in [0, 1, 2]]\n                hyps.append(pred_tokens)\n\n    # Tính BLEU\n    bleu4 = corpus_bleu(refs, hyps, weights=(0.25, 0.25, 0.25, 0.25))\n    print(f\"BLEU-4 Score: {bleu4*100:.2f}\")\n\n# Gọi hàm đánh giá (lưu ý: chạy hơi lâu vì generation tuần tự)\nevaluate_bleu(model, test_loader, device, dataset.vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T15:51:47.589750Z","iopub.execute_input":"2025-12-09T15:51:47.590096Z","iopub.status.idle":"2025-12-09T15:51:52.049669Z","shell.execute_reply.started":"2025-12-09T15:51:47.590070Z","shell.execute_reply":"2025-12-09T15:51:52.048793Z"}},"outputs":[{"name":"stdout","text":"BLEU-4 Score: 7.00\n","output_type":"stream"}],"execution_count":24}]}