{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 13476335,
          "sourceType": "datasetVersion",
          "datasetId": 8555466
        }
      ],
      "dockerImageVersionId": 31154,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Full solution — E-ELAN backbone → Object Detection (Faster R-CNN) using Roboflow dataset\n",
        "\n",
        "**Nội dung:**\n",
        "\n",
        "- Triển khai E-ELAN (minh họa) và bọc nó thành backbone cho Faster R-CNN.\n",
        "- Dùng **Roboflow** API để tải dataset (YOLOv7 export), chuyển YOLO labels -> per-image JSON\n",
        "- Huấn luyện Faster R-CNN trên dataset nhỏ, lưu weights, nạp weights, chạy inference trên thư mục ảnh, in ra boxes + scores\n",
        "- Tính **precision** và **recall** dựa trên so khớp IoU (threshold configurable)\n",
        "- Vẽ đồ thị biến thiên loss trong quá trình train\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "KdhhY-eeyzBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imports, seed, device"
      ],
      "metadata": {
        "id": "S8gx9uTayzBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device:', DEVICE)\n",
        "print(torch.__version__)\n",
        "print(f\"Name of GPU: {torch.cuda.get_device_name(DEVICE)}\")"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1HG6tluyzBY",
        "outputId": "a4774382-9121-4cc0-922e-1a26a4f74817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "2.8.0+cu126\n",
            "Name of GPU: Tesla T4\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. E-ELAN (lightweight educational implementation)"
      ],
      "metadata": {
        "id": "aqcM7utqyzBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBNAct(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k=3, stride=1, padding=None, groups=1):\n",
        "        super().__init__()\n",
        "        if padding is None:\n",
        "            padding = (k - 1) // 2\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, k, stride, padding=padding, groups=groups, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_ch)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "\n",
        "class EELANBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, expansion=2, n_branches=4):\n",
        "        super().__init__()\n",
        "        cexp = in_channels * expansion\n",
        "        self.expand = ConvBNAct(in_channels, cexp, k=1)\n",
        "        csplit = cexp // n_branches\n",
        "        self.branches = nn.ModuleList([ConvBNAct(csplit, csplit, k=3) for _ in range(n_branches)])\n",
        "        self.fuse = ConvBNAct(cexp, out_channels, k=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.expand(x)\n",
        "        splits = torch.chunk(x, len(self.branches), dim=1)\n",
        "        outs = [b(s) for b, s in zip(self.branches, splits)]\n",
        "        x = torch.cat(outs, dim=1)\n",
        "        x = self.fuse(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "b = EELANBlock(16, 32)\n",
        "xx = torch.randn(2, 16, 64, 64)\n",
        "yy = b(xx)\n",
        "print('E-ELAN output shape:', yy.shape)\n",
        "print(b)"
      ],
      "metadata": {
        "trusted": true,
        "id": "sQt91wO5yzBY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d84cc6-cae4-4c11-80f5-f29d180de232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E-ELAN output shape: torch.Size([2, 32, 64, 64])\n",
            "EELANBlock(\n",
            "  (expand): ConvBNAct(\n",
            "    (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (act): ReLU(inplace=True)\n",
            "  )\n",
            "  (branches): ModuleList(\n",
            "    (0-3): 4 x ConvBNAct(\n",
            "      (conv): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (fuse): ConvBNAct(\n",
            "    (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (act): ReLU(inplace=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Wrap E-ELAN as backbone for Faster R-CNN"
      ],
      "metadata": {
        "id": "K_k3Sc_HyzBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "\n",
        "class BackboneWrapper(nn.Module):\n",
        "    def __init__(self, eelan_backbone: nn.Module, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.backbone = eelan_backbone\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.backbone(x)\n",
        "        # Expected return: OrderedDict[str, Tensor]\n",
        "        return OrderedDict([('0', feat)])\n",
        "\n",
        "\n",
        "def build_detector_with_eelan(eelan_backbone, backbone_out_channels, num_classes):\n",
        "    backbone = BackboneWrapper(eelan_backbone, backbone_out_channels)\n",
        "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "    model = FasterRCNN(backbone, num_classes=num_classes, rpn_anchor_generator=anchor_generator)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Build a small backbone for demonstration (ensure output stride isn't too small)\n",
        "small_backbone = nn.Sequential(ConvBNAct(3, 32), EELANBlock(32, 64))\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs — DataParallel on E-ELAN backbone only.\")\n",
        "    small_backbone = torch.nn.DataParallel(small_backbone)\n",
        "else:\n",
        "    print(\"Using single GPU or CPU.\")\n",
        "det_model = build_detector_with_eelan(small_backbone, backbone_out_channels=64, num_classes=31).to(\n",
        "    DEVICE)  # num_classes includes background\n",
        "print(det_model)"
      ],
      "metadata": {
        "trusted": true,
        "id": "8cNUFi1FyzBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20272593-5ee5-4dbe-978f-d03d2c8ef6ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using single GPU or CPU.\n",
            "FasterRCNN(\n",
            "  (transform): GeneralizedRCNNTransform(\n",
            "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
            "  )\n",
            "  (backbone): BackboneWrapper(\n",
            "    (backbone): Sequential(\n",
            "      (0): ConvBNAct(\n",
            "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): EELANBlock(\n",
            "        (expand): ConvBNAct(\n",
            "          (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "        (branches): ModuleList(\n",
            "          (0-3): 4 x ConvBNAct(\n",
            "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): ReLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (fuse): ConvBNAct(\n",
            "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (rpn): RegionProposalNetwork(\n",
            "    (anchor_generator): AnchorGenerator()\n",
            "    (head): RPNHead(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (cls_logits): Conv2d(64, 15, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (bbox_pred): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): RoIHeads(\n",
            "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
            "    (box_head): TwoMLPHead(\n",
            "      (fc6): Linear(in_features=3136, out_features=1024, bias=True)\n",
            "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (box_predictor): FastRCNNPredictor(\n",
            "      (cls_score): Linear(in_features=1024, out_features=31, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=124, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "\n",
        "rf = Roboflow(api_key=\"wdM97i7Q3ORIQiEJN8JL\")\n",
        "project = rf.workspace(\"cropdataset\").project(\"plant-doc-dgqyu\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov7\")\n",
        "\n",
        "dataset_dir = Path(dataset.location) if hasattr(dataset, 'location') else Path(\"plant-doc\")\n",
        "print(f\"Dataset downloaded to: {dataset_dir.resolve()}\")\n",
        "\n",
        "\n",
        "def yolo_to_coco_json_split(split_dir: Path):\n",
        "    img_dir = split_dir / \"images\"\n",
        "    label_dir = split_dir / \"labels\"\n",
        "    ann_dir = split_dir / \"annotations\"\n",
        "    ann_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    converted = 0\n",
        "    # Use glob to find all txt files first\n",
        "    label_files = sorted(label_dir.glob(\"*.txt\"))\n",
        "    # Generate sequential filenames for JSON\n",
        "    json_stems = [f\"image_{i:06d}\" for i in range(len(label_files))]\n",
        "\n",
        "    for i, txt_path in enumerate(label_files):\n",
        "        stem = txt_path.stem\n",
        "        # tìm ảnh tương ứng (có thể jpg hoặc png)\n",
        "        img_path = img_dir / f\"{stem}.jpg\"\n",
        "        if not img_path.exists():\n",
        "            img_path = img_dir / f\"{stem}.png\"\n",
        "        if not img_path.exists():\n",
        "            continue\n",
        "\n",
        "        with Image.open(img_path) as img:\n",
        "            w, h = img.size\n",
        "\n",
        "        boxes, labels = [], []\n",
        "        with open(txt_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) != 5:\n",
        "                    continue\n",
        "                cls_id, xc, yc, bw, bh = map(float, parts)\n",
        "                # Chuyển tọa độ YOLO sang pixel tuyệt đối\n",
        "                x1 = (xc - bw / 2) * w\n",
        "                y1 = (yc - bh / 2) * h\n",
        "                x2 = (xc + bw / 2) * w\n",
        "                y2 = (yc + bh / 2) * h\n",
        "                boxes.append([x1, y1, x2, y2])\n",
        "                labels.append(int(cls_id) + 1)  # +1 vì background = 0\n",
        "\n",
        "        # Use the generated short stem for the JSON filename\n",
        "        ann = {\"boxes\": boxes, \"labels\": labels}\n",
        "        with open(ann_dir / f\"{json_stems[i]}.json\", \"w\") as f:\n",
        "            json.dump(ann, f)\n",
        "        converted += 1\n",
        "\n",
        "    print(f\"Converted {converted} annotations in {split_dir}\")\n",
        "\n",
        "\n",
        "for split in [\"train\", \"valid\", \"test\"]:\n",
        "    split_path = dataset_dir / split\n",
        "    if split_path.exists():\n",
        "        yolo_to_coco_json_split(split_path)\n",
        "    else:\n",
        "        print(f\" Split '{split}' not found, skipping...\")\n",
        "print(\"Done converting all available splits!\")"
      ],
      "metadata": {
        "id": "ZCINE66W1Mmi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ffb316-dbf1-4b42-c04d-95c01e1cbc4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.10.5)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.60.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n",
            "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m142.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in plant-doc-1 to yolov7pytorch:: 100%|██████████| 128358/128358 [00:08<00:00, 15588.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to plant-doc-1 in yolov7pytorch:: 100%|██████████| 5146/5146 [00:00<00:00, 6146.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to: /content/plant-doc-1\n",
            "Converted 1926 annotations in /content/plant-doc-1/train\n",
            "Converted 513 annotations in /content/plant-doc-1/valid\n",
            "Converted 128 annotations in /content/plant-doc-1/test\n",
            "Done converting all available splits!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Detection dataset class and dataloader helper"
      ],
      "metadata": {
        "id": "N-_EuKv-yzBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDetectionDataset(Dataset):\n",
        "    def __init__(self, root: str, split: str = 'train', transforms=None):\n",
        "        self.root = Path(root)\n",
        "        self.split = split\n",
        "        self.img_dir = self.root / \"images\"  # Point to the images directory within the split\n",
        "        self.ann_dir = self.root / 'annotations'  # Point to annotations directory within the split\n",
        "        self.json_files = sorted(self.ann_dir.glob('image_*.json'))  # Load short JSON filenames\n",
        "        if not self.json_files:\n",
        "            raise ValueError(f\"No JSON annotation files found in {self.ann_dir}. Check the directory structure.\")\n",
        "        self.transforms = transforms\n",
        "        # self.num_classes = 3  # Adjust based on your PlantDoc classes (13+ likely) # TODO: Dynamically get num_classes\n",
        "\n",
        "        # Dynamically determine the number of classes\n",
        "        all_labels = []\n",
        "        for json_path in self.json_files:\n",
        "            with open(json_path, 'r') as f:\n",
        "                j = json.load(f)\n",
        "                all_labels.extend(j['labels'])\n",
        "        self.num_classes = max(all_labels) + 1 if all_labels else 1  # +1 for background\n",
        "\n",
        "        print(\n",
        "            f\"Dataset for split '{split}' found {len(self.json_files)} annotation files with {self.num_classes} classes (including background).\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.json_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        json_path = self.json_files[idx]\n",
        "        idd = json_path.stem  # e.g., \"image_000001\"\n",
        "\n",
        "        # Find corresponding image (jpg or png) - based on the mapping created during conversion\n",
        "        # Assuming a simple sequential mapping for now. A more robust solution might store the original filename.\n",
        "        # For this fix, we'll assume the image files are also sequentially ordered or can be matched by index.\n",
        "        # Given the conversion generated \"image_000001.json\" from the first .txt, we'll look for the first image.\n",
        "        # This is a potential point of failure if image and label files aren't in the same order.\n",
        "        # A better approach would be to save the original filename in the JSON or use a mapping.\n",
        "        # For now, let's try to find the image based on the sequential index.\n",
        "        img_stem_prefix = idd.replace('image_', '')\n",
        "        img_options = sorted(self.img_dir.glob(f\"{img_stem_prefix}.*\"))  # Try matching the sequential number\n",
        "\n",
        "        if not img_options:\n",
        "            # Fallback: if sequential matching fails, try to find any image that corresponds to the original stem\n",
        "            # This part needs the original stem, which is not available in the JSON.\n",
        "            # Let's stick to the sequential assumption for now and add a more explicit error if image is not found by sequential index.\n",
        "            img_files_in_dir = sorted(self.img_dir.glob(\"*.*\"))\n",
        "            if idx < len(img_files_in_dir):\n",
        "                img_path = img_files_in_dir[idx]  # Assume sequential order\n",
        "            else:\n",
        "                raise FileNotFoundError(\n",
        "                    f\"Could not find corresponding image for {json_path.name} in {self.img_dir}. Sequential mapping failed.\")\n",
        "        else:\n",
        "            img_path = img_options[0]\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        image = transforms.ToTensor()(image)\n",
        "\n",
        "        # Load JSON annotation\n",
        "        with open(json_path, 'r') as f:\n",
        "            j = json.load(f)\n",
        "        boxes = torch.tensor(j['boxes'], dtype=torch.float32)\n",
        "        labels = torch.tensor(j['labels'], dtype=torch.int64)\n",
        "\n",
        "        # Filter out boxes with zero width or height\n",
        "        keep = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n",
        "        boxes = boxes[keep]\n",
        "        labels = labels[keep]\n",
        "\n",
        "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([idx])}\n",
        "        if self.transforms:\n",
        "            # Note: torchvision transforms for detection models expect image and target as a tuple\n",
        "            image, target = self.transforms(image, target)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def collate_fn_detection(batch):\n",
        "    images, targets = list(zip(*batch))\n",
        "    return list(images), list(targets)"
      ],
      "metadata": {
        "trusted": true,
        "id": "1j3-DT9CyzBb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training & evaluation for detection (Faster R-CNN)"
      ],
      "metadata": {
        "id": "cY_va-1JyzBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.ops import box_iou\n",
        "\n",
        "\n",
        "def compute_iou_ap(gt_boxes_list, pred_boxes_list, pred_scores_list, iou_thresh=0.5):\n",
        "    all_precisions = []\n",
        "    all_recalls = []\n",
        "    for gt_boxes, pred_boxes, scores in zip(gt_boxes_list, pred_boxes_list, pred_scores_list):\n",
        "        if pred_boxes.numel() == 0:\n",
        "            tp = 0;\n",
        "            fp = 0;\n",
        "            fn = gt_boxes.size(0)\n",
        "        else:\n",
        "            ious = box_iou(pred_boxes, gt_boxes) if gt_boxes.numel() > 0 else torch.empty((pred_boxes.size(0), 0))\n",
        "            P, G = pred_boxes.size(0), gt_boxes.size(0)\n",
        "            matched_gt = set();\n",
        "            tp = 0;\n",
        "            fp = 0\n",
        "            for i in range(P):\n",
        "                if G == 0:\n",
        "                    fp += 1\n",
        "                    continue\n",
        "                iou_row = ious[i]\n",
        "                best_iou, best_idx = torch.max(iou_row, dim=0)\n",
        "                if float(best_iou) >= iou_thresh and int(best_idx) not in matched_gt:\n",
        "                    tp += 1\n",
        "                    matched_gt.add(int(best_idx))\n",
        "                else:\n",
        "                    fp += 1\n",
        "            fn = G - len(matched_gt)\n",
        "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "        all_precisions.append(prec)\n",
        "        all_recalls.append(rec)\n",
        "    return float(np.mean(all_precisions)), float(np.mean(all_recalls))\n",
        "\n",
        "\n",
        "def train_detection_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    iters = 0\n",
        "\n",
        "    for images, targets in data_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += float(losses)\n",
        "        iters += 1\n",
        "\n",
        "    avg_loss = running_loss / max(1, iters)\n",
        "    print(f\"Epoch {epoch} — Average Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_detection(model, data_loader, device, iou_thresh=0.5):\n",
        "    model.eval()\n",
        "    gt_boxes_list, pred_boxes_list, pred_scores_list = [], [], []\n",
        "    for images, targets in data_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        outputs = model(images)\n",
        "        for t, out in zip(targets, outputs):\n",
        "            gt_boxes = t[\"boxes\"].cpu()\n",
        "            pred_boxes = out[\"boxes\"].detach().cpu()\n",
        "            scores = out[\"scores\"].detach().cpu()\n",
        "            gt_boxes_list.append(gt_boxes)\n",
        "            pred_boxes_list.append(pred_boxes)\n",
        "            pred_scores_list.append(scores)\n",
        "    prec, rec = compute_iou_ap(gt_boxes_list, pred_boxes_list, pred_scores_list, iou_thresh)\n",
        "    print(f\"Evaluation: Precision={prec:.4f}, Recall={rec:.4f} (IoU>{iou_thresh})\")\n",
        "    return prec, rec"
      ],
      "metadata": {
        "trusted": true,
        "id": "vg6VvLD_yzBb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ROOT = '/content/plant-doc-1'\n",
        "\n",
        "# Removed manual transforms. The model's internal transform handles resizing and ToTensor.\n",
        "# transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
        "\n",
        "# Initialize datasets\n",
        "# Pass transforms=None as the model's internal transform will be used.\n",
        "train_ds = SimpleDetectionDataset(f\"{DATA_ROOT}/train\", transforms=None)\n",
        "val_ds = SimpleDetectionDataset(f\"{DATA_ROOT}/valid\", transforms=None)\n",
        "test_ds = SimpleDetectionDataset(f\"{DATA_ROOT}/test\", transforms=None)  # thêm test\n",
        "\n",
        "# Get the number of classes from the training dataset\n",
        "num_classes = train_ds.num_classes\n",
        "print(f\"Number of classes determined from training dataset: {num_classes}\")\n",
        "\n",
        "# Rebuild the detection model with the correct number of classes\n",
        "# (Assuming small_backbone and build_detector_with_eelan are defined in previous cells and accessible)\n",
        "det_model = build_detector_with_eelan(small_backbone, backbone_out_channels=64, num_classes=num_classes).to(DEVICE)\n",
        "print(\"Rebuilt detector model with correct number of classes:\")\n",
        "print(det_model)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "optimizer = torch.optim.SGD(det_model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "num_epochs = 10\n",
        "\n",
        "history = {'train_loss': [], 'val_prec': [], 'val_rec': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    t0 = time.time()\n",
        "    train_loss = train_detection_one_epoch(det_model, optimizer, train_loader, DEVICE, epoch)\n",
        "    prec, rec = evaluate_detection(det_model, val_loader, DEVICE, iou_thresh=0.5)\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_prec'].append(prec)\n",
        "    history['val_rec'].append(rec)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} completed in {time.time() - t0:.1f}s\")\n",
        "    print(f\"\\t Train loss: {train_loss:.4f} | Val Precision: {prec:.4f} | Val Recall: {rec:.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    Path(\"weights\").mkdir(exist_ok=True)\n",
        "    torch.save({'model_state_dict': det_model.state_dict()},\n",
        "               f'weights/det_eelan_epoch{epoch + 1}.pth')\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], marker='o', color='tab:red', label='Train Loss')\n",
        "plt.title('Training Loss per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['val_prec'], marker='s', label='Validation Precision', color='tab:blue')\n",
        "plt.plot(history['val_rec'], marker='^', label='Validation Recall', color='tab:green')\n",
        "plt.title('Validation Precision/Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Evaluating on Test Set...\")\n",
        "prec_test, rec_test = evaluate_detection(det_model, test_loader, DEVICE, iou_thresh=0.5)\n",
        "print(f\"Test Precision: {prec_test:.4f}, Test Recall: {rec_test:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "2m5G01fuyzBb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8578e438-37d1-42b8-df0c-1772afee1e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset for split 'train' found 1926 annotation files with 31 classes (including background).\n",
            "Dataset for split 'train' found 513 annotation files with 31 classes (including background).\n",
            "Dataset for split 'train' found 128 annotation files with 31 classes (including background).\n",
            "Number of classes determined from training dataset: 31\n",
            "Rebuilt detector model with correct number of classes:\n",
            "FasterRCNN(\n",
            "  (transform): GeneralizedRCNNTransform(\n",
            "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
            "  )\n",
            "  (backbone): BackboneWrapper(\n",
            "    (backbone): Sequential(\n",
            "      (0): ConvBNAct(\n",
            "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): EELANBlock(\n",
            "        (expand): ConvBNAct(\n",
            "          (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "        (branches): ModuleList(\n",
            "          (0-3): 4 x ConvBNAct(\n",
            "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): ReLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (fuse): ConvBNAct(\n",
            "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (act): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (rpn): RegionProposalNetwork(\n",
            "    (anchor_generator): AnchorGenerator()\n",
            "    (head): RPNHead(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (cls_logits): Conv2d(64, 15, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (bbox_pred): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): RoIHeads(\n",
            "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
            "    (box_head): TwoMLPHead(\n",
            "      (fc6): Linear(in_features=3136, out_features=1024, bias=True)\n",
            "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (box_predictor): FastRCNNPredictor(\n",
            "      (cls_score): Linear(in_features=1024, out_features=31, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=124, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 3.01 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.61 GiB is free. Process 2719 has 12.13 GiB memory in use. Of the allocated memory 10.97 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3536700364.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_detection_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2609881609.py\u001b[0m in \u001b[0;36mtrain_detection_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         detections = self.transform.postprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"targets should not be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatched_gt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_targets_to_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0mregression_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_coder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatched_gt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             loss_objectness, loss_rpn_box_reg = self.compute_loss(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36massign_targets_to_anchors\u001b[0;34m(self, anchors, targets)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mlabels_per_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors_per_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mmatch_quality_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors_per_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0mmatched_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_matcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_quality_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0;31m# get the targets corresponding GT for each proposal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36mbox_iou\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_iou\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0minter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_box_inter_union\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m     \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minter\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0munion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0miou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36m_box_inter_union\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mrb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [N,M,2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mwh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [N,M,2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0minter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# [N,M]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.01 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.61 GiB is free. Process 2719 has 12.13 GiB memory in use. Of the allocated memory 10.97 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Inference: load saved weights, run on folder of images, print predictions; compute precision & recall"
      ],
      "metadata": {
        "id": "PKz5nfcJyzBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_detector_weights(model, path, device='cpu'):\n",
        "    if path and os.path.exists(path):\n",
        "        ckpt = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        print(f'Loaded weights from {path}')\n",
        "        return True\n",
        "    else:\n",
        "        print(f'No weights found at {path}');\n",
        "        return False\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_folder_detector(model, folder, device='cpu', score_thresh=0.5):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    results = []\n",
        "    for p in Path(folder).glob('*'):\n",
        "        if p.suffix.lower() not in ['.jpg', '.png', '.jpeg']: continue\n",
        "        img = Image.open(p).convert('RGB')\n",
        "        x = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
        "        outputs = model(list(x))[0]\n",
        "        boxes = outputs['boxes'].cpu()\n",
        "        scores = outputs['scores'].cpu()\n",
        "        labels = outputs.get('labels', None)\n",
        "        for b, s in zip(boxes, scores):\n",
        "            if float(s) < score_thresh: continue\n",
        "            results.append({'image': str(p), 'box': b.tolist(), 'score': float(s)})\n",
        "        print(f\"{p.name}: {len(results)} detections (score>{score_thresh}) — sample top: {results[:3]}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "def infer_and_metrics_detector(model, images_folder, annotations_folder, device='cpu', score_thresh=0.5,\n",
        "                               iou_thresh=0.5):\n",
        "    model = model.to(device);\n",
        "    model.eval()\n",
        "    gt_boxes_list = [];\n",
        "    pred_boxes_list = [];\n",
        "    pred_scores_list = []\n",
        "    for img_path in Path(images_folder).glob('*'):\n",
        "        if img_path.suffix.lower() not in ['.jpg', '.png', '.jpeg']: continue\n",
        "        stem = img_path.stem\n",
        "        ann_path = Path(annotations_folder) / (stem + '.json')\n",
        "        if not ann_path.exists(): continue\n",
        "        with open(ann_path, 'r') as f:\n",
        "            j = json.load(f)\n",
        "        gt_boxes = torch.tensor(j['boxes'], dtype=torch.float32)\n",
        "        # run model\n",
        "        x = transforms.ToTensor()(Image.open(img_path).convert('RGB')).unsqueeze(0).to(device)\n",
        "        out = model(list(x))[0]\n",
        "        pred_boxes = out['boxes'].detach().cpu()\n",
        "        scores = out['scores'].detach().cpu()\n",
        "        # filter by score_thresh\n",
        "        keep = scores >= score_thresh\n",
        "        if keep.numel() == 0:\n",
        "            pred_boxes = torch.empty((0, 4))\n",
        "            scores = torch.empty((0,))\n",
        "        else:\n",
        "            pred_boxes = pred_boxes[keep]\n",
        "            scores = scores[keep]\n",
        "        gt_boxes_list.append(gt_boxes)\n",
        "        pred_boxes_list.append(pred_boxes)\n",
        "        pred_scores_list.append(scores)\n",
        "    prec, rec = compute_iou_ap(gt_boxes_list, pred_boxes_list, pred_scores_list, iou_thresh)\n",
        "    print(f'Overall precision={prec:.4f}, recall={rec:.4f} at score>={score_thresh} and IoU>={iou_thresh}')\n",
        "    return prec, rec\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "z3RMV9opyzBc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "weights_path = 'weights/det_eelan_epoch20.pth'\n",
        "load_detector_weights(det_model, weights_path, device=DEVICE)\n",
        "\n",
        "TEST_IMG_DIR = 'Playing-Cards-1/test/images'\n",
        "TEST_ANN_DIR = 'Playing-Cards-1/test/annotations'\n",
        "\n",
        "print(\"\\n Running inference on test set...\")\n",
        "results = infer_folder_detector(det_model, TEST_IMG_DIR, device=DEVICE, score_thresh=0.5)\n",
        "\n",
        "if os.path.exists(TEST_ANN_DIR):\n",
        "    print(\"\\n Evaluating Precision & Recall...\")\n",
        "    prec, rec = infer_and_metrics_detector(det_model, TEST_IMG_DIR, TEST_ANN_DIR,\n",
        "                                           device=DEVICE, score_thresh=0.5, iou_thresh=0.5)\n",
        "    print(f\"Final Test Precision={prec:.4f}, Recall={rec:.4f}\")\n",
        "else:\n",
        "    print(\"No annotation folder found for test set — skipping metric computation.\")\n",
        "\n",
        "\n",
        "def visualize_detections(results, num_images=3):\n",
        "    shown = 0\n",
        "    for res in results:\n",
        "        if shown >= num_images:\n",
        "            break\n",
        "        img_path = res['image']\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        box = res['box']\n",
        "        draw.rectangle(box, outline='red', width=3)\n",
        "        draw.text((box[0], box[1] - 10), f\"{res['score']:.2f}\", fill='red')\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"{Path(img_path).name} | score={res['score']:.2f}\")\n",
        "        plt.show()\n",
        "        shown += 1\n",
        "\n",
        "\n",
        "print(\"\\n Visualizing sample detections...\")\n",
        "visualize_detections(results, num_images=10)"
      ],
      "metadata": {
        "id": "FB6XUPX072kZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}